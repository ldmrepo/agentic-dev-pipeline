# ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œë°œ ì›Œí¬í”Œë¡œìš°

## ğŸ“Š End-to-End ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìë™ êµ¬ì¶•

ì´ ì›Œí¬í”Œë¡œìš°ëŠ” ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ë¶„ì„ ë° ì‹œê°í™”ê¹Œì§€ ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ìë™ìœ¼ë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.

**ëª©í‘œ ì™„ë£Œ ì‹œê°„: 2-3ì‹œê°„**

## ì‹¤í–‰ ë°©ë²•
```bash
export REQUIREMENTS="e-commerce ì‚¬ìš©ì í–‰ë™ ë°ì´í„° íŒŒì´í”„ë¼ì¸ (ì‹¤ì‹œê°„ ë¶„ì„ + ëŒ€ì‹œë³´ë“œ)"
export DATA_SOURCES="web_analytics,user_events,transaction_data"
export PROCESSING_TYPE="batch,streaming" # ë˜ëŠ” "batch" ë˜ëŠ” "streaming"
claude -f workflows/data-pipeline-development.md
```

## ì›Œí¬í”Œë¡œìš° ë‹¨ê³„

### ğŸ¯ 1ë‹¨ê³„: ë°ì´í„° ì•„í‚¤í…ì²˜ ì„¤ê³„ (30ë¶„)

ë‹¤ìŒì„ ìˆ˜í–‰í•´ì¤˜:

#### ë°ì´í„° í”Œë¡œìš° ì„¤ê³„ ë° ìš”êµ¬ì‚¬í•­ ë¶„ì„
- íŒŒì´í”„ë¼ì¸ ìš”êµ¬ì‚¬í•­: ${REQUIREMENTS}
- ë°ì´í„° ì†ŒìŠ¤: ${DATA_SOURCES}
- ì²˜ë¦¬ ë°©ì‹: ${PROCESSING_TYPE}

```
ë°ì´í„° ì†ŒìŠ¤ ë¶„ì„:
1. ë°ì´í„° ì†ŒìŠ¤ ì¢…ë¥˜ ë° íŠ¹ì„± íŒŒì•…
   - êµ¬ì¡°í™” ë°ì´í„°: ë°ì´í„°ë² ì´ìŠ¤, CSV, JSON
   - ë°˜êµ¬ì¡°í™” ë°ì´í„°: ë¡œê·¸ íŒŒì¼, XML, YAML
   - ë¹„êµ¬ì¡°í™” ë°ì´í„°: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤

2. ë°ì´í„° ë³¼ë¥¨ ë° ì†ë„ ì¶”ì •
   - ì¼ì¼ ë°ì´í„° ë³¼ë¥¨ (GB/TB)
   - ì‹¤ì‹œê°„ ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ (ì´ë²¤íŠ¸/ì´ˆ)
   - í”¼í¬ ì‹œê°„ëŒ€ íŠ¸ë˜í”½ íŒ¨í„´

3. ë°ì´í„° í’ˆì§ˆ ë° ê±°ë²„ë„ŒìŠ¤
   - ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
   - ë°ì´í„° ë¬´ê²°ì„± ê·œì¹™
   - ê°œì¸ì •ë³´ ë³´í˜¸ ìš”êµ¬ì‚¬í•­ (GDPR, CCPA)
   - ë°ì´í„° ë³´ì¡´ ì •ì±…

ë°ì´í„° ì²˜ë¦¬ ì „ëµ:
ë°°ì¹˜ ì²˜ë¦¬ (Batch Processing):
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì¼ê´„ ì²˜ë¦¬
- ë³µì¡í•œ ì§‘ê³„ ë° ë¶„ì„ ì‘ì—…
- ì •í™•ì„±ì´ ì†ë„ë³´ë‹¤ ì¤‘ìš”í•œ ê²½ìš°
- ë„êµ¬: Apache Spark, Apache Airflow

ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (Stream Processing):
- ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
- ì¦‰ì‹œ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
- ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜
- ë„êµ¬: Apache Kafka, Apache Flink

í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ (Lambda Architecture):
- ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¼ì˜ ì¥ì  ê²°í•©
- ì‹¤ì‹œê°„ + ì¼ê´„ ì²˜ë¦¬ ë³‘í–‰
- ì •í™•ì„±ê³¼ ì†ë„ ëª¨ë‘ í™•ë³´
```

#### ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ ë° ì•„í‚¤í…ì²˜ ì„¤ê³„
```
Modern Data Stack êµ¬ì„±:

ë°ì´í„° ìˆ˜ì§‘ (Ingestion):
- Apache Kafka: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°
- Apache NiFi: ë°ì´í„° í”Œë¡œìš° ê´€ë¦¬
- Airbyte: ELT ë„êµ¬
- Fivetran: SaaS ë°ì´í„° ì»¤ë„¥í„°

ë°ì´í„° ì €ì¥ (Storage):
- Data Lake: Amazon S3, Google Cloud Storage
- Data Warehouse: Snowflake, BigQuery, Redshift
- NoSQL: MongoDB, Cassandra
- ì‹œê³„ì—´ DB: InfluxDB, TimescaleDB

ë°ì´í„° ì²˜ë¦¬ (Processing):
- Apache Spark: ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬
- Apache Flink: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
- dbt: ë°ì´í„° ë³€í™˜ ë° ëª¨ë¸ë§
- Great Expectations: ë°ì´í„° í’ˆì§ˆ ê²€ì¦

ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Orchestration):
- Apache Airflow: ì›Œí¬í”Œë¡œìš° ê´€ë¦¬
- Prefect: ìµœì‹  ì›Œí¬í”Œë¡œìš° ì—”ì§„
- Dagster: ë°ì´í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

ëª¨ë‹ˆí„°ë§ & ê´€ì°°ì„±:
- DataDog: ì¸í”„ë¼ ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§
- Monte Carlo: ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- Grafana: ë©”íŠ¸ë¦­ ì‹œê°í™”

í´ë¼ìš°ë“œ ê¸°ë°˜ ì•„í‚¤í…ì²˜:
AWS ìŠ¤íƒ:
- ìˆ˜ì§‘: Kinesis, MSK (Kafka)
- ì €ì¥: S3, Redshift, RDS
- ì²˜ë¦¬: EMR (Spark), Lambda
- ë¶„ì„: QuickSight, SageMaker

GCP ìŠ¤íƒ:
- ìˆ˜ì§‘: Pub/Sub, Dataflow
- ì €ì¥: Cloud Storage, BigQuery
- ì²˜ë¦¬: Dataproc, Cloud Functions
- ë¶„ì„: Looker, Vertex AI

Azure ìŠ¤íƒ:
- ìˆ˜ì§‘: Event Hubs, Stream Analytics
- ì €ì¥: Data Lake Storage, Synapse
- ì²˜ë¦¬: HDInsight, Functions
- ë¶„ì„: Power BI, Machine Learning
```

**ê²°ê³¼ë¬¼**: 
- `docs/data-architecture.md`
- `docs/data-flow-diagram.md`
- `architecture/data-pipeline-design.yaml`

---

### ğŸ”§ 2ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ êµ¬ì¶• (45ë¶„)

ë‹¤ìŒ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ (Kafka + Kafka Connect)
```
Kafka í´ëŸ¬ìŠ¤í„° ì„¤ì •:
# docker-compose.yml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  kafka-connect:
    image: confluentinc/cp-kafka-connect:latest
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "connect-status"

í† í”½ ìƒì„± ë° êµ¬ì„±:
# í† í”½ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic user-events \
  --partitions 3 \
  --replication-factor 1

kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic transaction-data \
  --partitions 6 \
  --replication-factor 1

Kafka Connect ì»¤ë„¥í„° ì„¤ì •:
# ë°ì´í„°ë² ì´ìŠ¤ ì†ŒìŠ¤ ì»¤ë„¥í„°
{
  "name": "mysql-source-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url": "jdbc:mysql://mysql:3306/ecommerce",
    "connection.user": "kafka_user",
    "connection.password": "kafka_password",
    "table.whitelist": "users,orders,products",
    "mode": "incrementing",
    "incrementing.column.name": "id",
    "topic.prefix": "mysql-"
  }
}

# ì›¹ ë¡œê·¸ ì»¤ë„¥í„°
{
  "name": "file-source-connector",
  "config": {
    "connector.class": "FileStreamSource",
    "file": "/var/log/web/access.log",
    "topic": "web-logs"
  }
}
```

#### Python ë°ì´í„° ìˆ˜ì§‘ê¸° êµ¬í˜„
```python
# data_collectors/event_collector.py
import json
import asyncio
from kafka import KafkaProducer
from typing import Dict, Any
import logging

class EventCollector:
    def __init__(self, kafka_config: Dict[str, Any]):
        self.producer = KafkaProducer(
            bootstrap_servers=kafka_config['bootstrap_servers'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None
        )
        self.logger = logging.getLogger(__name__)
    
    async def collect_user_events(self, event_data: Dict[str, Any]):
        """ì‚¬ìš©ì ì´ë²¤íŠ¸ ìˆ˜ì§‘"""
        try:
            # ì´ë²¤íŠ¸ ë°ì´í„° ê²€ì¦
            validated_event = self._validate_event(event_data)
            
            # ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
            masked_event = self._mask_pii(validated_event)
            
            # Kafkaë¡œ ì „ì†¡
            self.producer.send(
                topic='user-events',
                key=event_data.get('user_id'),
                value=masked_event
            )
            
            self.logger.info(f"Event collected: {event_data.get('event_type')}")
            
        except Exception as e:
            self.logger.error(f"Failed to collect event: {e}")
    
    def _validate_event(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """ì´ë²¤íŠ¸ ë°ì´í„° ê²€ì¦"""
        required_fields = ['event_type', 'timestamp', 'user_id']
        
        for field in required_fields:
            if field not in event:
                raise ValueError(f"Missing required field: {field}")
        
        return event
    
    def _mask_pii(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹"""
        pii_fields = ['email', 'phone', 'ip_address']
        
        for field in pii_fields:
            if field in event:
                event[field] = self._hash_field(event[field])
        
        return event

# API ì„œë²„ í†µí•©
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel

app = FastAPI()
collector = EventCollector({'bootstrap_servers': ['localhost:9092']})

class UserEvent(BaseModel):
    event_type: str
    user_id: str
    timestamp: int
    properties: Dict[str, Any]

@app.post("/events")
async def collect_event(event: UserEvent, background_tasks: BackgroundTasks):
    background_tasks.add_task(
        collector.collect_user_events, 
        event.dict()
    )
    return {"status": "accepted"}
```

#### ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘ (Airbyte)
```yaml
# airbyte-config.yaml
version: "0.40.0"
definitions:
  sources:
    postgres-source:
      type: postgres
      config:
        host: postgres-db
        port: 5432
        database: ecommerce_prod
        username: readonly_user
        password: ${POSTGRES_PASSWORD}
        schemas: ["public"]
        ssl: true
    
    api-source:
      type: http-api
      config:
        base_url: "https://api.example.com"
        headers:
          Authorization: "Bearer ${API_TOKEN}"
        endpoints:
          - path: "/users"
            method: "GET"
            pagination:
              type: "offset"
              limit: 1000
          - path: "/orders"
            method: "GET"
            pagination:
              type: "cursor"
              cursor_field: "created_at"

  destinations:
    data-warehouse:
      type: snowflake
      config:
        host: ${SNOWFLAKE_HOST}
        database: ANALYTICS
        schema: RAW_DATA
        username: ${SNOWFLAKE_USER}
        password: ${SNOWFLAKE_PASSWORD}
        warehouse: COMPUTE_WH

connections:
  - source: postgres-source
    destination: data-warehouse
    schedule: "0 2 * * *"  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
    
  - source: api-source
    destination: data-warehouse
    schedule: "0 */6 * * *"  # 6ì‹œê°„ë§ˆë‹¤
```

---

### âš™ï¸ 3ë‹¨ê³„: ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜ (1ì‹œê°„)

ë‹¤ìŒ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### Apache Spark ë°°ì¹˜ ì²˜ë¦¬
```python
# spark_jobs/user_behavior_analysis.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import logging

class UserBehaviorAnalyzer:
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("UserBehaviorAnalysis") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        self.logger = logging.getLogger(__name__)
    
    def analyze_daily_user_behavior(self, date: str):
        """ì¼ì¼ ì‚¬ìš©ì í–‰ë™ ë¶„ì„"""
        
        # ì›ì‹œ ì´ë²¤íŠ¸ ë°ì´í„° ë¡œë“œ
        events_df = self.spark.read \
            .option("multiline", "true") \
            .json(f"s3a://data-lake/events/date={date}/*")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        events_df = self._validate_data_quality(events_df)
        
        # ì‚¬ìš©ì ì„¸ì…˜ ë¶„ì„
        user_sessions = self._analyze_user_sessions(events_df)
        
        # ì œí’ˆ ìƒí˜¸ì‘ìš© ë¶„ì„
        product_interactions = self._analyze_product_interactions(events_df)
        
        # ì „í™˜ ê¹”ë•Œê¸° ë¶„ì„
        conversion_funnel = self._analyze_conversion_funnel(events_df)
        
        # ê²°ê³¼ ì €ì¥
        self._save_analysis_results(date, {
            'user_sessions': user_sessions,
            'product_interactions': product_interactions,
            'conversion_funnel': conversion_funnel
        })
    
    def _analyze_user_sessions(self, events_df):
        """ì‚¬ìš©ì ì„¸ì…˜ ë¶„ì„"""
        
        # ì„¸ì…˜ ì •ì˜: 30ë¶„ ì´ìƒ ë¹„í™œì„± ì‹œ ìƒˆ ì„¸ì…˜
        window_spec = Window.partitionBy("user_id").orderBy("timestamp")
        
        sessions_df = events_df \
            .withColumn("prev_timestamp", lag("timestamp").over(window_spec)) \
            .withColumn("time_diff", 
                       col("timestamp") - col("prev_timestamp")) \
            .withColumn("session_break", 
                       when(col("time_diff") > 1800, 1).otherwise(0)) \
            .withColumn("session_id", 
                       sum("session_break").over(window_spec))
        
        # ì„¸ì…˜ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        session_metrics = sessions_df \
            .groupBy("user_id", "session_id") \
            .agg(
                min("timestamp").alias("session_start"),
                max("timestamp").alias("session_end"),
                count("*").alias("event_count"),
                countDistinct("page_url").alias("unique_pages"),
                sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("purchases")
            ) \
            .withColumn("session_duration", 
                       col("session_end") - col("session_start"))
        
        return session_metrics
    
    def _validate_data_quality(self, df):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ["user_id", "timestamp", "event_type"]
        for field in required_fields:
            if field not in df.columns:
                raise ValueError(f"Missing required field: {field}")
        
        # NULL ê°’ ì œê±°
        df_cleaned = df.dropna(subset=required_fields)
        
        # ì´ìƒì¹˜ ì œê±° (íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„ í™•ì¸)
        current_time = unix_timestamp()
        df_cleaned = df_cleaned.filter(
            (col("timestamp") > current_time - 86400 * 30) &  # 30ì¼ ì´ë‚´
            (col("timestamp") <= current_time)
        )
        
        # ë¡œê·¸ ê¸°ë¡
        original_count = df.count()
        cleaned_count = df_cleaned.count()
        self.logger.info(f"Data quality check: {original_count} â†’ {cleaned_count} records")
        
        return df_cleaned

# dbt ë°ì´í„° ë³€í™˜ ëª¨ë¸
# models/marts/user_behavior/daily_user_metrics.sql
{{ config(materialized='table') }}

WITH user_daily_events AS (
    SELECT 
        user_id,
        DATE(timestamp) as event_date,
        COUNT(*) as total_events,
        COUNT(DISTINCT session_id) as sessions,
        SUM(CASE WHEN event_type = 'page_view' THEN 1 ELSE 0 END) as page_views,
        SUM(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as add_to_carts,
        SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchases,
        SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as total_revenue
    FROM {{ ref('stg_user_events') }}
    WHERE timestamp >= CURRENT_DATE - INTERVAL '7 days'
    GROUP BY user_id, DATE(timestamp)
),

user_metrics AS (
    SELECT 
        *,
        CASE 
            WHEN purchases > 0 THEN total_revenue / purchases 
            ELSE 0 
        END as avg_order_value,
        CASE 
            WHEN add_to_carts > 0 THEN purchases::FLOAT / add_to_carts 
            ELSE 0 
        END as cart_conversion_rate
    FROM user_daily_events
)

SELECT * FROM user_metrics
```

#### ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (Apache Flink)
```python
# flink_jobs/real_time_analytics.py
import json
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.common.serialization import SimpleStringSchema

class RealTimeAnalytics:
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.env.set_parallelism(4)
        
        self.table_env = StreamTableEnvironment.create(self.env)
        
        # Kafka ì»¤ë„¥í„° ì„¤ì •
        self._setup_kafka_connectors()
    
    def _setup_kafka_connectors(self):
        """Kafka ì†ŒìŠ¤ ë° ì‹±í¬ ì„¤ì •"""
        
        # ì‚¬ìš©ì ì´ë²¤íŠ¸ ì†ŒìŠ¤ í…Œì´ë¸”
        self.table_env.execute_sql("""
            CREATE TABLE user_events (
                user_id STRING,
                event_type STRING,
                timestamp BIGINT,
                page_url STRING,
                product_id STRING,
                revenue DECIMAL(10,2),
                event_time AS TO_TIMESTAMP(FROM_UNIXTIME(timestamp)),
                WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND
            ) WITH (
                'connector' = 'kafka',
                'topic' = 'user-events',
                'properties.bootstrap.servers' = 'kafka:9092',
                'properties.group.id' = 'flink-analytics',
                'format' = 'json',
                'scan.startup.mode' = 'latest-offset'
            )
        """)
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì‹±í¬ í…Œì´ë¸”
        self.table_env.execute_sql("""
            CREATE TABLE real_time_metrics (
                window_start TIMESTAMP(3),
                window_end TIMESTAMP(3),
                total_events BIGINT,
                unique_users BIGINT,
                total_revenue DECIMAL(10,2),
                avg_session_duration DOUBLE
            ) WITH (
                'connector' = 'kafka',
                'topic' = 'real-time-metrics',
                'properties.bootstrap.servers' = 'kafka:9092',
                'format' = 'json'
            )
        """)
    
    def run_real_time_analytics(self):
        """ì‹¤ì‹œê°„ ë¶„ì„ ì‹¤í–‰"""
        
        # 5ë¶„ ë‹¨ìœ„ ìœˆë„ìš° ì§‘ê³„
        self.table_env.execute_sql("""
            INSERT INTO real_time_metrics
            SELECT 
                TUMBLE_START(event_time, INTERVAL '5' MINUTE) as window_start,
                TUMBLE_END(event_time, INTERVAL '5' MINUTE) as window_end,
                COUNT(*) as total_events,
                COUNT(DISTINCT user_id) as unique_users,
                SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as total_revenue,
                AVG(CASE WHEN event_type = 'session_end' THEN timestamp ELSE NULL END) as avg_session_duration
            FROM user_events
            WHERE event_time BETWEEN 
                TUMBLE_START(event_time, INTERVAL '5' MINUTE) AND 
                TUMBLE_END(event_time, INTERVAL '5' MINUTE)
            GROUP BY TUMBLE(event_time, INTERVAL '5' MINUTE)
        """)
        
        # ì´ìƒ íƒì§€ - ê¸‰ê²©í•œ íŠ¸ë˜í”½ ì¦ê°€
        self.table_env.execute_sql("""
            CREATE VIEW traffic_anomalies AS
            SELECT *
            FROM (
                SELECT 
                    window_start,
                    total_events,
                    LAG(total_events, 1) OVER (ORDER BY window_start) as prev_events,
                    (total_events - LAG(total_events, 1) OVER (ORDER BY window_start)) / 
                    LAG(total_events, 1) OVER (ORDER BY window_start) * 100 as growth_rate
                FROM real_time_metrics
            ) 
            WHERE growth_rate > 200  -- 200% ì´ìƒ ì¦ê°€ ì‹œ ì´ìƒìœ¼ë¡œ íŒë‹¨
        """)

    def setup_alerting(self):
        """ì‹¤ì‹œê°„ ì•Œë¦¼ ì„¤ì •"""
        
        # ìˆ˜ìµ ê¸‰ê° ì•Œë¦¼
        self.table_env.execute_sql("""
            CREATE TABLE revenue_alerts (
                alert_time TIMESTAMP(3),
                message STRING,
                severity STRING,
                current_revenue DECIMAL(10,2),
                expected_revenue DECIMAL(10,2)
            ) WITH (
                'connector' = 'kafka',
                'topic' = 'alerts',
                'properties.bootstrap.servers' = 'kafka:9092',
                'format' = 'json'
            )
        """)
        
        self.table_env.execute_sql("""
            INSERT INTO revenue_alerts
            SELECT 
                CURRENT_TIMESTAMP as alert_time,
                CONCAT('Revenue drop detected: ', CAST(total_revenue AS STRING)) as message,
                'HIGH' as severity,
                total_revenue as current_revenue,
                LAG(total_revenue, 1) OVER (ORDER BY window_start) as expected_revenue
            FROM real_time_metrics
            WHERE total_revenue < LAG(total_revenue, 1) OVER (ORDER BY window_start) * 0.5
        """)
```

---

### ğŸ—„ï¸ 4ë‹¨ê³„: ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë° ë ˆì´í¬ êµ¬ì¶• (30ë¶„)

ë‹¤ìŒ ë°ì´í„° ì €ì¥ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### ë°ì´í„° ë ˆì´í¬ êµ¬ì¡° (S3 ê¸°ë°˜)
```
ë°ì´í„° ë ˆì´í¬ ê³„ì¸µ êµ¬ì¡°:
s3://company-data-lake/
â”œâ”€â”€ raw/                    # ì›ì‹œ ë°ì´í„° (Bronze Layer)
â”‚   â”œâ”€â”€ events/
â”‚   â”‚   â””â”€â”€ year=2024/month=03/day=15/hour=14/
â”‚   â”œâ”€â”€ transactions/
â”‚   â””â”€â”€ user_profiles/
â”œâ”€â”€ processed/              # ì •ì œëœ ë°ì´í„° (Silver Layer)
â”‚   â”œâ”€â”€ user_sessions/
â”‚   â”œâ”€â”€ product_interactions/
â”‚   â””â”€â”€ daily_aggregates/
â”œâ”€â”€ curated/               # ë¶„ì„ìš© ë°ì´í„° (Gold Layer)
â”‚   â”œâ”€â”€ user_behavior_metrics/
â”‚   â”œâ”€â”€ sales_analytics/
â”‚   â””â”€â”€ ml_features/
â””â”€â”€ archive/               # ì•„ì¹´ì´ë¸Œ ë°ì´í„°
    â””â”€â”€ year=2023/

íŒŒí‹°ì…”ë‹ ì „ëµ:
- ì‹œê°„ ê¸°ë°˜: year/month/day/hour
- ì§€ì—­ ê¸°ë°˜: region/country
- ì‚¬ìš©ì ê¸°ë°˜: user_segment

ë°ì´í„° í¬ë§·:
- Raw: JSON, CSV, Avro
- Processed: Parquet (ì»¬ëŸ¼í˜• ì €ì¥)
- Curated: Delta Lake (ë²„ì „ ê´€ë¦¬)
```

#### Snowflake ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì„¤ê³„
```sql
-- ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°
CREATE DATABASE ANALYTICS;
USE DATABASE ANALYTICS;

-- ìŠ¤í‚¤ë§ˆ ìƒì„±
CREATE SCHEMA RAW_DATA;      -- ì›ì‹œ ë°ì´í„°
CREATE SCHEMA STAGING;       -- ì¤‘ê°„ ì²˜ë¦¬ ë°ì´í„°
CREATE SCHEMA MARTS;         -- ë¶„ì„ìš© ë°ì´í„° ë§ˆíŠ¸
CREATE SCHEMA SANDBOX;       -- ì‹¤í—˜ìš© ê³µê°„

-- íŒ©íŠ¸ í…Œì´ë¸”: ì‚¬ìš©ì ì´ë²¤íŠ¸
CREATE OR REPLACE TABLE MARTS.FACT_USER_EVENTS (
    event_id STRING PRIMARY KEY,
    user_id STRING NOT NULL,
    session_id STRING,
    event_type STRING NOT NULL,
    event_timestamp TIMESTAMP_NTZ NOT NULL,
    page_url STRING,
    product_id STRING,
    revenue DECIMAL(10,2),
    properties VARIANT,
    created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
) 
CLUSTER BY (event_timestamp, user_id)
COMMENT = 'ì‚¬ìš©ì ì´ë²¤íŠ¸ íŒ©íŠ¸ í…Œì´ë¸”';

-- ë””ë©˜ì ¼ í…Œì´ë¸”: ì‚¬ìš©ì
CREATE OR REPLACE TABLE MARTS.DIM_USERS (
    user_id STRING PRIMARY KEY,
    email STRING,
    first_name STRING,
    last_name STRING,
    registration_date DATE,
    user_segment STRING,
    lifetime_value DECIMAL(10,2),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
    updated_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
)
COMMENT = 'ì‚¬ìš©ì ë””ë©˜ì ¼ í…Œì´ë¸”';

-- ì§‘ê³„ í…Œì´ë¸”: ì¼ì¼ ì‚¬ìš©ì ë©”íŠ¸ë¦­
CREATE OR REPLACE TABLE MARTS.AGG_DAILY_USER_METRICS (
    metric_date DATE,
    user_id STRING,
    total_events INTEGER,
    session_count INTEGER,
    page_views INTEGER,
    add_to_carts INTEGER,
    purchases INTEGER,
    total_revenue DECIMAL(10,2),
    avg_session_duration DECIMAL(10,2),
    PRIMARY KEY (metric_date, user_id)
)
CLUSTER BY (metric_date)
COMMENT = 'ì¼ì¼ ì‚¬ìš©ì í–‰ë™ ë©”íŠ¸ë¦­';

-- ìŠ¤íŠ¸ë¦¼ ìƒì„± (CDC)
CREATE OR REPLACE STREAM USER_EVENTS_STREAM 
ON TABLE RAW_DATA.USER_EVENTS
COMMENT = 'ì‚¬ìš©ì ì´ë²¤íŠ¸ ë³€ê²½ ìŠ¤íŠ¸ë¦¼';

-- íƒœìŠ¤í¬ ìƒì„± (ìë™ ì²˜ë¦¬)
CREATE OR REPLACE TASK PROCESS_USER_EVENTS
    WAREHOUSE = ANALYTICS_WH
    SCHEDULE = '5 MINUTE'
    WHEN SYSTEM$STREAM_HAS_DATA('USER_EVENTS_STREAM')
AS
    INSERT INTO MARTS.FACT_USER_EVENTS
    SELECT 
        event_id,
        user_id,
        session_id,
        event_type,
        event_timestamp,
        page_url,
        product_id,
        revenue,
        properties,
        CURRENT_TIMESTAMP()
    FROM RAW_DATA.USER_EVENTS_STREAM
    WHERE METADATA$ACTION = 'INSERT';

-- ë°ì´í„° ë³´ì¡´ ì •ì±…
ALTER TABLE MARTS.FACT_USER_EVENTS 
SET DATA_RETENTION_TIME_IN_DAYS = 90;

-- ë³´ì•ˆ ì„¤ì •
CREATE ROW ACCESS POLICY user_data_policy AS (user_id) RETURNS BOOLEAN ->
    CASE 
        WHEN CURRENT_ROLE() IN ('ADMIN', 'DATA_ENGINEER') THEN TRUE
        WHEN CURRENT_ROLE() = 'ANALYST' AND user_id NOT LIKE '%@internal.com' THEN TRUE
        ELSE FALSE
    END;

ALTER TABLE MARTS.DIM_USERS ADD ROW ACCESS POLICY user_data_policy ON (user_id);
```

#### ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ë° í’ˆì§ˆ ê´€ë¦¬
```python
# data_quality/great_expectations_config.py
import great_expectations as ge
from great_expectations.checkpoint import SimpleCheckpoint

class DataQualityValidator:
    def __init__(self, data_context_config):
        self.context = ge.DataContext(data_context_config)
    
    def create_expectation_suite(self, suite_name: str):
        """ë°ì´í„° í’ˆì§ˆ ê·œì¹™ ì •ì˜"""
        
        suite = self.context.create_expectation_suite(
            expectation_suite_name=suite_name
        )
        
        # ì‚¬ìš©ì ì´ë²¤íŠ¸ ë°ì´í„° í’ˆì§ˆ ê·œì¹™
        if suite_name == "user_events_quality":
            expectations = [
                {
                    "expectation_type": "expect_column_to_exist",
                    "kwargs": {"column": "user_id"}
                },
                {
                    "expectation_type": "expect_column_values_to_not_be_null",
                    "kwargs": {"column": "user_id"}
                },
                {
                    "expectation_type": "expect_column_values_to_match_regex",
                    "kwargs": {
                        "column": "event_type",
                        "regex": "^(page_view|add_to_cart|purchase|login|logout)$"
                    }
                },
                {
                    "expectation_type": "expect_column_values_to_be_between",
                    "kwargs": {
                        "column": "timestamp",
                        "min_value": 1640995200,  # 2022-01-01
                        "max_value": None  # í˜„ì¬ ì‹œê°„ê¹Œì§€
                    }
                },
                {
                    "expectation_type": "expect_column_values_to_be_of_type",
                    "kwargs": {
                        "column": "revenue",
                        "type_": "float"
                    }
                }
            ]
            
            for expectation in expectations:
                suite.add_expectation(**expectation)
        
        return suite
    
    def run_validation(self, batch_request, suite_name: str):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰"""
        
        checkpoint_config = {
            "name": f"{suite_name}_checkpoint",
            "config_version": 1.0,
            "template_name": None,
            "run_name_template": "%Y%m%d-%H%M%S-my-run-name-template",
            "expectation_suite_name": suite_name,
            "batch_request": batch_request,
            "action_list": [
                {
                    "name": "store_validation_result",
                    "action": {"class_name": "StoreValidationResultAction"},
                },
                {
                    "name": "update_data_docs",
                    "action": {"class_name": "UpdateDataDocsAction"},
                },
                {
                    "name": "send_slack_notification",
                    "action": {
                        "class_name": "SlackNotificationAction",
                        "slack_webhook": "https://hooks.slack.com/...",
                        "notify_on": "failure"
                    }
                }
            ],
        }
        
        checkpoint = SimpleCheckpoint(
            f"{suite_name}_checkpoint",
            self.context,
            **checkpoint_config
        )
        
        return checkpoint.run()

# ë°ì´í„° ì¹´íƒˆë¡œê·¸ (Apache Atlas)
from atlas_client.client import Atlas

class DataCatalog:
    def __init__(self, atlas_config):
        self.atlas = Atlas(
            host=atlas_config['host'],
            port=atlas_config['port'],
            username=atlas_config['username'],
            password=atlas_config['password']
        )
    
    def register_dataset(self, dataset_info):
        """ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ë“±ë¡"""
        
        entity = {
            "typeName": "DataSet",
            "attributes": {
                "name": dataset_info['name'],
                "description": dataset_info['description'],
                "owner": dataset_info['owner'],
                "location": dataset_info['location'],
                "schema": dataset_info['schema'],
                "tags": dataset_info['tags'],
                "qualifiedName": f"{dataset_info['name']}@data-lake"
            }
        }
        
        return self.atlas.entity_post.create(entity)
```

---

### ğŸ“Š 5ë‹¨ê³„: ë¶„ì„ ë° ì‹œê°í™” (30ë¶„)

ë‹¤ìŒ ë¶„ì„ ë° ì‹œê°í™” ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ ëŒ€ì‹œë³´ë“œ (Tableau/PowerBI)
```python
# dashboards/bi_dashboard_config.py
from tableau_api_lib import TableauServerConnection
import json

class BIDashboardBuilder:
    def __init__(self, tableau_config):
        self.tableau = TableauServerConnection(tableau_config)
        self.tableau.sign_in()
    
    def create_user_behavior_dashboard(self):
        """ì‚¬ìš©ì í–‰ë™ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        
        dashboard_config = {
            "name": "User Behavior Analytics",
            "sheets": [
                {
                    "name": "Daily Active Users",
                    "data_source": "MARTS.AGG_DAILY_USER_METRICS",
                    "chart_type": "line",
                    "dimensions": ["metric_date"],
                    "measures": ["unique_users"],
                    "filters": ["metric_date >= DATEADD('day', -30, CURRENT_DATE())"]
                },
                {
                    "name": "Conversion Funnel",
                    "data_source": "MARTS.CONVERSION_FUNNEL",
                    "chart_type": "funnel",
                    "dimensions": ["funnel_step"],
                    "measures": ["user_count", "conversion_rate"]
                },
                {
                    "name": "Revenue Trends",
                    "data_source": "MARTS.REVENUE_ANALYTICS",
                    "chart_type": "combo",
                    "dimensions": ["date"],
                    "measures": ["total_revenue", "avg_order_value"]
                },
                {
                    "name": "User Segments",
                    "data_source": "MARTS.USER_SEGMENTS",
                    "chart_type": "treemap",
                    "dimensions": ["segment_name"],
                    "measures": ["user_count", "revenue_contribution"]
                }
            ]
        }
        
        return self._create_dashboard(dashboard_config)
    
    def create_real_time_monitoring_dashboard(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"""
        
        dashboard_config = {
            "name": "Real-Time Monitoring",
            "refresh_interval": 60,  # 60ì´ˆë§ˆë‹¤ ìƒˆë¡œê³ ì¹¨
            "sheets": [
                {
                    "name": "Live Traffic",
                    "data_source": "real_time_metrics",
                    "chart_type": "area",
                    "streaming": True
                },
                {
                    "name": "Current Active Users",
                    "data_source": "active_sessions",
                    "chart_type": "metric",
                    "kpi_threshold": {
                        "good": "> 1000",
                        "warning": "500-1000",
                        "critical": "< 500"
                    }
                }
            ]
        }
        
        return self._create_dashboard(dashboard_config)

# Grafana ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ
# dashboards/grafana_dashboard.json
{
  "dashboard": {
    "title": "Data Pipeline Monitoring",
    "panels": [
      {
        "id": 1,
        "title": "Kafka Topic Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(kafka_server_brokertopicmetrics_messagesin_total[5m])) by (topic)",
            "legendFormat": "{{topic}}"
          }
        ]
      },
      {
        "id": 2,
        "title": "Spark Job Performance",
        "type": "table",
        "targets": [
          {
            "expr": "spark_job_duration_seconds",
            "format": "table"
          }
        ]
      },
      {
        "id": 3,
        "title": "Data Quality Metrics",
        "type": "stat",
        "targets": [
          {
            "expr": "data_quality_validation_success_rate"
          }
        ]
      },
      {
        "id": 4,
        "title": "Pipeline Latency",
        "type": "gauge",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, pipeline_processing_duration_seconds_bucket)"
          }
        ]
      }
    ]
  }
}
```

#### ML ê¸°ë°˜ ë¶„ì„ (Python)
```python
# analytics/ml_analytics.py
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from prophet import Prophet
import mlflow

class MLAnalytics:
    def __init__(self, warehouse_connection):
        self.warehouse = warehouse_connection
        mlflow.set_tracking_uri("http://mlflow:5000")
    
    def anomaly_detection(self, date_range):
        """ì´ìƒ íƒì§€ ëª¨ë¸"""
        
        # ë°ì´í„° ë¡œë“œ
        query = f"""
        SELECT 
            user_id,
            DATE(event_timestamp) as date,
            COUNT(*) as event_count,
            SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as daily_revenue
        FROM MARTS.FACT_USER_EVENTS
        WHERE event_timestamp BETWEEN '{date_range[0]}' AND '{date_range[1]}'
        GROUP BY user_id, DATE(event_timestamp)
        """
        
        df = pd.read_sql(query, self.warehouse)
        
        # Isolation Forest ëª¨ë¸ í•™ìŠµ
        features = ['event_count', 'daily_revenue']
        model = IsolationForest(contamination=0.05, random_state=42)
        
        # ì´ìƒì¹˜ íƒì§€
        df['anomaly'] = model.fit_predict(df[features])
        df['anomaly_score'] = model.score_samples(df[features])
        
        # ê²°ê³¼ ì €ì¥
        anomalies = df[df['anomaly'] == -1]
        
        with mlflow.start_run():
            mlflow.log_metric("total_anomalies", len(anomalies))
            mlflow.sklearn.log_model(model, "anomaly_detector")
        
        return anomalies
    
    def revenue_forecasting(self, historical_days=365):
        """ìˆ˜ìµ ì˜ˆì¸¡ ëª¨ë¸"""
        
        # ê³¼ê±° ë°ì´í„° ë¡œë“œ
        query = f"""
        SELECT 
            DATE(event_timestamp) as ds,
            SUM(revenue) as y
        FROM MARTS.FACT_USER_EVENTS
        WHERE event_type = 'purchase'
            AND event_timestamp >= CURRENT_DATE - {historical_days}
        GROUP BY DATE(event_timestamp)
        ORDER BY ds
        """
        
        df = pd.read_sql(query, self.warehouse)
        
        # Prophet ëª¨ë¸ í•™ìŠµ
        model = Prophet(
            daily_seasonality=True,
            weekly_seasonality=True,
            yearly_seasonality=True,
            changepoint_prior_scale=0.05
        )
        
        model.fit(df)
        
        # 30ì¼ ì˜ˆì¸¡
        future = model.make_future_dataframe(periods=30)
        forecast = model.predict(future)
        
        # ëª¨ë¸ ì €ì¥
        with mlflow.start_run():
            mlflow.prophet.log_model(model, "revenue_forecast")
            mlflow.log_metric("mape", self._calculate_mape(df, forecast))
        
        return forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
    
    def user_segmentation(self):
        """ì‚¬ìš©ì ì„¸ë¶„í™”"""
        
        query = """
        SELECT 
            user_id,
            COUNT(DISTINCT DATE(event_timestamp)) as active_days,
            COUNT(DISTINCT session_id) as total_sessions,
            SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchase_count,
            SUM(CASE WHEN event_type = 'purchase' THEN revenue ELSE 0 END) as total_revenue,
            DATEDIFF('day', MIN(event_timestamp), MAX(event_timestamp)) as customer_lifetime
        FROM MARTS.FACT_USER_EVENTS
        GROUP BY user_id
        """
        
        df = pd.read_sql(query, self.warehouse)
        
        # RFM ë¶„ì„
        df['recency'] = df['customer_lifetime']
        df['frequency'] = df['purchase_count'] 
        df['monetary'] = df['total_revenue']
        
        # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        df['segment'] = pd.cut(
            df['total_revenue'],
            bins=[0, 100, 500, 1000, float('inf')],
            labels=['Bronze', 'Silver', 'Gold', 'Platinum']
        )
        
        return df
```

---

### ğŸš€ 6ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë° ëª¨ë‹ˆí„°ë§ (45ë¶„)

ë‹¤ìŒ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### Apache Airflow DAG êµ¬ì„±
```python
# airflow/dags/data_pipeline_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from airflow.providers.spark.operators.spark_submit import SparkSubmitOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'sla': timedelta(hours=2)
}

# ë©”ì¸ íŒŒì´í”„ë¼ì¸ DAG
dag = DAG(
    'data_pipeline',
    default_args=default_args,
    description='End-to-end data pipeline',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
    start_date=days_ago(1),
    catchup=False,
    tags=['production', 'data-pipeline']
)

# 1. ë°ì´í„° ìˆ˜ì§‘
collect_kafka_data = BashOperator(
    task_id='collect_kafka_data',
    bash_command='python /opt/data_collectors/kafka_collector.py --date {{ ds }}',
    dag=dag
)

collect_database_data = BashOperator(
    task_id='collect_database_data', 
    bash_command='airbyte sync --connection-id {{ var.value.db_connection_id }}',
    dag=dag
)

# 2. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
validate_data_quality = PythonOperator(
    task_id='validate_data_quality',
    python_callable=run_data_quality_checks,
    op_kwargs={'date': '{{ ds }}'},
    dag=dag
)

# 3. Spark ì²˜ë¦¬
spark_batch_processing = SparkSubmitOperator(
    task_id='spark_batch_processing',
    application='/opt/spark_jobs/user_behavior_analysis.py',
    conf={
        'spark.executor.memory': '4g',
        'spark.executor.cores': '4',
        'spark.sql.adaptive.enabled': 'true'
    },
    application_args=['--date', '{{ ds }}'],
    dag=dag
)

# 4. dbt ë³€í™˜
dbt_transformation = BashOperator(
    task_id='dbt_transformation',
    bash_command='cd /opt/dbt && dbt run --models +daily_user_metrics',
    dag=dag
)

# 5. Snowflake ì ì¬
load_to_warehouse = SnowflakeOperator(
    task_id='load_to_warehouse',
    snowflake_conn_id='snowflake_default',
    sql="""
        COPY INTO MARTS.FACT_USER_EVENTS
        FROM @S3_STAGE/processed/{{ ds }}/
        FILE_FORMAT = (TYPE = PARQUET)
        ON_ERROR = CONTINUE;
    """,
    dag=dag
)

# 6. ML ëª¨ë¸ ì‹¤í–‰
run_ml_models = PythonOperator(
    task_id='run_ml_models',
    python_callable=execute_ml_analytics,
    op_kwargs={'date': '{{ ds }}'},
    dag=dag
)

# 7. ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
refresh_dashboards = BashOperator(
    task_id='refresh_dashboards',
    bash_command='python /opt/dashboards/refresh_all.py',
    dag=dag
)

# 8. ì•Œë¦¼ ì „ì†¡
send_completion_notification = PythonOperator(
    task_id='send_notification',
    python_callable=send_slack_notification,
    op_kwargs={
        'message': 'Data pipeline completed successfully for {{ ds }}',
        'channel': '#data-alerts'
    },
    trigger_rule='all_done',
    dag=dag
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
[collect_kafka_data, collect_database_data] >> validate_data_quality
validate_data_quality >> spark_batch_processing
spark_batch_processing >> dbt_transformation
dbt_transformation >> load_to_warehouse
load_to_warehouse >> [run_ml_models, refresh_dashboards]
[run_ml_models, refresh_dashboards] >> send_completion_notification

# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° DAG
streaming_dag = DAG(
    'streaming_pipeline',
    default_args=default_args,
    description='Real-time streaming pipeline',
    schedule_interval=None,  # í•­ìƒ ì‹¤í–‰
    start_date=days_ago(1),
    catchup=False,
    tags=['production', 'streaming']
)

start_flink_job = BashOperator(
    task_id='start_flink_job',
    bash_command="""
        flink run \
            -c com.example.RealTimeAnalytics \
            /opt/flink_jobs/real-time-analytics.jar
    """,
    dag=streaming_dag
)
```

#### ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ
```python
# monitoring/pipeline_monitor.py
import requests
from prometheus_client import Counter, Histogram, Gauge
import logging
from typing import Dict, Any

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
pipeline_success_counter = Counter('pipeline_success_total', 'Total successful pipeline runs')
pipeline_failure_counter = Counter('pipeline_failure_total', 'Total failed pipeline runs')
pipeline_duration_histogram = Histogram('pipeline_duration_seconds', 'Pipeline execution duration')
data_quality_score_gauge = Gauge('data_quality_score', 'Current data quality score')

class PipelineMonitor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.slack_webhook = config['slack_webhook']
        self.pagerduty_key = config['pagerduty_key']
    
    def monitor_pipeline_health(self):
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
        
        health_checks = {
            'kafka_health': self._check_kafka_health(),
            'spark_health': self._check_spark_health(),
            'warehouse_health': self._check_warehouse_health(),
            'data_freshness': self._check_data_freshness()
        }
        
        # ê±´ê°• ìƒíƒœ í‰ê°€
        overall_health = all(health_checks.values())
        
        if not overall_health:
            self._send_alert({
                'severity': 'critical',
                'message': 'Pipeline health check failed',
                'details': health_checks
            })
        
        return health_checks
    
    def _check_data_freshness(self):
        """ë°ì´í„° ìµœì‹ ì„± í™•ì¸"""
        
        query = """
        SELECT MAX(event_timestamp) as latest_timestamp
        FROM MARTS.FACT_USER_EVENTS
        """
        
        result = self.warehouse.execute(query)
        latest_timestamp = result[0]['latest_timestamp']
        
        # 1ì‹œê°„ ì´ìƒ ì§€ì—°ë˜ë©´ ë¬¸ì œë¡œ íŒë‹¨
        delay_minutes = (datetime.now() - latest_timestamp).total_seconds() / 60
        
        if delay_minutes > 60:
            self._send_alert({
                'severity': 'warning',
                'message': f'Data freshness issue: {delay_minutes:.0f} minutes delay',
                'metric': 'data_freshness_delay_minutes',
                'value': delay_minutes
            })
            return False
        
        return True
    
    def _send_alert(self, alert: Dict[str, Any]):
        """ì•Œë¦¼ ì „ì†¡"""
        
        # Slack ì•Œë¦¼
        if alert['severity'] in ['critical', 'warning']:
            slack_message = {
                'text': f"ğŸš¨ {alert['severity'].upper()}: {alert['message']}",
                'attachments': [{
                    'color': 'danger' if alert['severity'] == 'critical' else 'warning',
                    'fields': [
                        {'title': k, 'value': str(v), 'short': True}
                        for k, v in alert.get('details', {}).items()
                    ]
                }]
            }
            
            requests.post(self.slack_webhook, json=slack_message)
        
        # PagerDuty ì•Œë¦¼ (Critical only)
        if alert['severity'] == 'critical':
            pagerduty_event = {
                'routing_key': self.pagerduty_key,
                'event_action': 'trigger',
                'payload': {
                    'summary': alert['message'],
                    'severity': 'critical',
                    'source': 'data-pipeline',
                    'custom_details': alert.get('details', {})
                }
            }
            
            requests.post(
                'https://events.pagerduty.com/v2/enqueue',
                json=pagerduty_event
            )

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
    
    def track_processing_metrics(self, job_name: str, metrics: Dict[str, Any]):
        """ì²˜ë¦¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì """
        
        # ì²˜ë¦¬ ì‹œê°„
        if 'duration_seconds' in metrics:
            pipeline_duration_histogram.observe(metrics['duration_seconds'])
        
        # ì²˜ë¦¬ëœ ë ˆì½”ë“œ ìˆ˜
        if 'records_processed' in metrics:
            records_gauge = Gauge(
                f'pipeline_{job_name}_records_processed',
                f'Records processed by {job_name}'
            )
            records_gauge.set(metrics['records_processed'])
        
        # ì—ëŸ¬ìœ¨
        if 'error_rate' in metrics:
            error_gauge = Gauge(
                f'pipeline_{job_name}_error_rate',
                f'Error rate for {job_name}'
            )
            error_gauge.set(metrics['error_rate'])
        
        # ì²˜ë¦¬ ì†ë„ (records/second)
        if 'throughput' in metrics:
            throughput_gauge = Gauge(
                f'pipeline_{job_name}_throughput',
                f'Processing throughput for {job_name}'
            )
            throughput_gauge.set(metrics['throughput'])
```

---

### ğŸ“‹ 7ë‹¨ê³„: ë¬¸ì„œí™” ë° ë°°í¬ (15ë¶„)

ë‹¤ìŒ ë¬¸ì„œì™€ ë°°í¬ ì„¤ì •ì„ ìƒì„±í•´ì¤˜:

#### íŒŒì´í”„ë¼ì¸ ë¬¸ì„œ
```markdown
# ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìš´ì˜ ê°€ì´ë“œ

## ì•„í‚¤í…ì²˜ ê°œìš”
- **ë°ì´í„° ìˆ˜ì§‘**: Kafka, Airbyte
- **ë°ì´í„° ì²˜ë¦¬**: Apache Spark, Apache Flink
- **ë°ì´í„° ì €ì¥**: S3 (Data Lake), Snowflake (Data Warehouse)
- **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: Apache Airflow
- **ëª¨ë‹ˆí„°ë§**: Prometheus, Grafana, DataDog

## ì¼ì¼ ìš´ì˜ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Airflow DAG ìƒíƒœ í™•ì¸
- [ ] ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ê²€í† 
- [ ] ì²˜ë¦¬ ì§€ì—° ëª¨ë‹ˆí„°ë§
- [ ] ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] ë¹„ìš© ëª¨ë‹ˆí„°ë§

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ë°ì´í„° ì§€ì—° ë¬¸ì œ
1. Kafka lag í™•ì¸: `kafka-consumer-groups --describe`
2. Spark job ìƒíƒœ í™•ì¸: Spark UI (http://spark-master:8080)
3. Airflow íƒœìŠ¤í¬ ë¡œê·¸ í™•ì¸

### ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ
1. Great Expectations ë¦¬í¬íŠ¸ í™•ì¸
2. ë°ì´í„° ì†ŒìŠ¤ ê²€ì¦
3. ë³€í™˜ ë¡œì§ ê²€í† 

### ì„±ëŠ¥ ë¬¸ì œ
1. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  í™•ì¸ (CPU, Memory, I/O)
2. íŒŒí‹°ì…˜ ì „ëµ ê²€í† 
3. ì¿¼ë¦¬ ìµœì í™”

## ì—°ë½ì²˜
- ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ íŒ€: data-eng@company.com
- ì˜¨ì½œ ì—”ì§€ë‹ˆì–´: #data-oncall (Slack)
```

#### ë°°í¬ ì„¤ì •
```yaml
# kubernetes/data-pipeline-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-pipeline
  namespace: data-platform
spec:
  replicas: 3
  selector:
    matchLabels:
      app: data-pipeline
  template:
    metadata:
      labels:
        app: data-pipeline
    spec:
      containers:
      - name: airflow-scheduler
        image: apache/airflow:2.5.0
        command: ["airflow", "scheduler"]
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
      - name: spark-worker
        image: bitnami/spark:3.3
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
```

**ê²°ê³¼ë¬¼**:
- ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜
- ì‹¤ì‹œê°„ ë° ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ
- ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì²´ê³„
- ML ê¸°ë°˜ ë¶„ì„ ê¸°ëŠ¥
- ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- ìë™í™”ëœ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

---

ì´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ë©´ ì™„ì „í•œ ì—”ë“œíˆ¬ì—”ë“œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ ìë™ìœ¼ë¡œ êµ¬ì¶•ë©ë‹ˆë‹¤!