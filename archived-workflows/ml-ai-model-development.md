# ML/AI ëª¨ë¸ ê°œë°œ ì›Œí¬í”Œë¡œìš°

## ğŸ¤– End-to-End ML/AI ëª¨ë¸ ê°œë°œ ìë™í™”

ì´ ì›Œí¬í”Œë¡œìš°ëŠ” ë°ì´í„° ì¤€ë¹„ë¶€í„° ëª¨ë¸ ë°°í¬ê¹Œì§€ ì™„ì „í•œ ML/AI ê°œë°œ íŒŒì´í”„ë¼ì¸ì„ ìë™ìœ¼ë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤.

**ëª©í‘œ ì™„ë£Œ ì‹œê°„: 3-4ì‹œê°„**

## ì‹¤í–‰ ë°©ë²•
```bash
export REQUIREMENTS="ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ (ì •í™•ë„ 90% ì´ìƒ, ì‹¤ì‹œê°„ ì˜ˆì¸¡ API)"
export MODEL_TYPE="classification" # classification, regression, clustering, nlp, computer_vision
export DATASET_SOURCE="s3://data-lake/customer_data/"
export DEPLOYMENT_TYPE="real-time" # real-time, batch, edge
claude -f workflows/ml-ai-model-development.md
```

## ì›Œí¬í”Œë¡œìš° ë‹¨ê³„

### ğŸ¯ 1ë‹¨ê³„: ML í”„ë¡œì íŠ¸ ì„¤ê³„ ë° ìš”êµ¬ì‚¬í•­ ë¶„ì„ (30ë¶„)

ë‹¤ìŒì„ ìˆ˜í–‰í•´ì¤˜:

#### ë¬¸ì œ ì •ì˜ ë° ëª©í‘œ ì„¤ì •
- ML ìš”êµ¬ì‚¬í•­: ${REQUIREMENTS}
- ëª¨ë¸ íƒ€ì…: ${MODEL_TYPE}
- ë°ì´í„° ì†ŒìŠ¤: ${DATASET_SOURCE}
- ë°°í¬ ë°©ì‹: ${DEPLOYMENT_TYPE}

```
ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ ë¶„ì„:
1. ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì™€ ML ëª©í‘œ ì •ë ¬
   - ì˜ˆì¸¡í•˜ë ¤ëŠ” íƒ€ê²Ÿ ì •ì˜
   - ì„±ê³µ ì§€í‘œ ì„¤ì • (ì •í™•ë„, ì¬í˜„ìœ¨, F1-score ë“±)
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì¸¡ì • ë°©ë²•

2. ì œì•½ì‚¬í•­ íŒŒì•…
   - ì˜ˆì¸¡ ì§€ì—°ì‹œê°„ ìš”êµ¬ì‚¬í•­
   - ëª¨ë¸ í•´ì„ê°€ëŠ¥ì„± í•„ìš” ì—¬ë¶€
   - ê·œì œ ì¤€ìˆ˜ ì‚¬í•­ (GDPR, ê³µì •ì„±)
   - ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ì œí•œ

3. í‰ê°€ ì „ëµ ìˆ˜ë¦½
   - Train/Validation/Test ë¶„í•  ì „ëµ
   - Cross-validation ë°©ë²•
   - A/B í…ŒìŠ¤íŠ¸ ê³„íš
   - ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ì§€í‘œ

ML ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜:
ë°ì´í„° íŒŒì´í”„ë¼ì¸:
- ë°ì´í„° ìˆ˜ì§‘ â†’ ì „ì²˜ë¦¬ â†’ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ â†’ ëª¨ë¸ í•™ìŠµ

ëª¨ë¸ ì„œë¹™ ì•„í‚¤í…ì²˜:
- ì‹¤ì‹œê°„: REST API / gRPC
- ë°°ì¹˜: Airflow + Spark
- ì—£ì§€: TensorFlow Lite / ONNX

MLOps ì¸í”„ë¼:
- ì‹¤í—˜ ì¶”ì : MLflow
- ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬: MLflow Model Registry
- ëª¨ë‹ˆí„°ë§: Prometheus + Grafana
- CI/CD: GitLab CI / GitHub Actions
```

#### ë°ì´í„° íƒìƒ‰ ë° ë¶„ì„ ê³„íš
```python
# EDA ì²´í¬ë¦¬ìŠ¤íŠ¸
1. ë°ì´í„° í’ˆì§ˆ í‰ê°€
   - ê²°ì¸¡ì¹˜ ë¶„ì„
   - ì´ìƒì¹˜ íƒì§€
   - ë°ì´í„° íƒ€ì… ê²€ì¦
   - ì¤‘ë³µ ë°ì´í„° í™•ì¸

2. í†µê³„ ë¶„ì„
   - ê¸°ìˆ  í†µê³„ëŸ‰
   - ë¶„í¬ ë¶„ì„
   - ìƒê´€ê´€ê³„ ë¶„ì„
   - íƒ€ê²Ÿ ë³€ìˆ˜ ë¶ˆê· í˜• í™•ì¸

3. ì‹œê°í™”
   - íˆìŠ¤í† ê·¸ë¨, ë°•ìŠ¤í”Œë¡¯
   - ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤
   - íˆíŠ¸ë§µ
   - ì‹œê³„ì—´ íŒ¨í„´ (ì‹œê³„ì—´ ë°ì´í„°ì˜ ê²½ìš°)

4. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê¸°íšŒ
   - íŒŒìƒ ë³€ìˆ˜ ìƒì„± ê°€ëŠ¥ì„±
   - ì°¨ì› ì¶•ì†Œ í•„ìš”ì„±
   - ì¸ì½”ë”© ì „ëµ
   - ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„±
```

**ê²°ê³¼ë¬¼**:
- `docs/ml-project-charter.md`
- `docs/data-exploration-plan.md`
- `architecture/ml-system-design.yaml`

---

### ğŸ“Š 2ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (1ì‹œê°„)

ë‹¤ìŒ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•´ì¤˜:

#### ë°ì´í„° ìˆ˜ì§‘ ë° ê²€ì¦
```python
# src/data/data_loader.py
import pandas as pd
import numpy as np
from typing import Tuple, Dict, Any
import great_expectations as ge
from sklearn.model_selection import train_test_split
import logging

class DataPipeline:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.ge_context = ge.DataContext()
    
    def load_data(self, data_path: str) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸° ê²€ì¦"""
        
        # ë°ì´í„° ë¡œë“œ
        if data_path.startswith('s3://'):
            df = pd.read_parquet(data_path)
        elif data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        else:
            df = pd.read_sql(data_path, self.config['db_connection'])
        
        self.logger.info(f"Loaded {len(df)} records from {data_path}")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        self._validate_data_quality(df)
        
        return df
    
    def _validate_data_quality(self, df: pd.DataFrame):
        """Great Expectationsë¥¼ ì‚¬ìš©í•œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        
        # Expectation Suite ìƒì„±
        suite = self.ge_context.create_expectation_suite(
            "ml_data_quality_suite",
            overwrite_existing=True
        )
        
        # ê¸°ë³¸ ê²€ì¦ ê·œì¹™
        validator = self.ge_context.get_validator(
            batch_request={"dataset": df},
            expectation_suite_name="ml_data_quality_suite"
        )
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        for column in self.config['required_columns']:
            validator.expect_column_to_exist(column)
            validator.expect_column_values_to_not_be_null(column)
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë²”ìœ„ í™•ì¸
        for column, bounds in self.config.get('numeric_bounds', {}).items():
            validator.expect_column_values_to_be_between(
                column, 
                min_value=bounds['min'], 
                max_value=bounds['max']
            )
        
        # ê²€ì¦ ì‹¤í–‰
        results = validator.validate()
        
        if not results["success"]:
            raise ValueError(f"Data quality validation failed: {results}")
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""
        
        # ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        if 'timestamp' in df.columns:
            df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
            df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        
        # ì§‘ê³„ íŠ¹ì„±
        if 'user_id' in df.columns:
            user_stats = df.groupby('user_id').agg({
                'transaction_amount': ['sum', 'mean', 'std', 'count'],
                'session_duration': ['mean', 'max']
            })
            user_stats.columns = ['_'.join(col) for col in user_stats.columns]
            df = df.merge(user_stats, on='user_id', how='left')
        
        # ë¹„ìœ¨ íŠ¹ì„±
        if 'page_views' in df.columns and 'purchases' in df.columns:
            df['conversion_rate'] = df['purchases'] / (df['page_views'] + 1)
        
        # ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©
        categorical_columns = df.select_dtypes(include=['object']).columns
        
        for col in categorical_columns:
            if df[col].nunique() < 20:  # Low cardinality
                df = pd.get_dummies(df, columns=[col], prefix=col)
            else:  # High cardinality
                # Target encoding
                if 'target' in df.columns:
                    target_mean = df.groupby(col)['target'].mean()
                    df[f'{col}_target_encoded'] = df[col].map(target_mean)
        
        return df
    
    def create_ml_dataset(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """MLìš© ë°ì´í„°ì…‹ ìƒì„±"""
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        target_column = self.config['target_column']
        feature_columns = [col for col in df.columns if col != target_column]
        
        X = df[feature_columns]
        y = df[target_column]
        
        # í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
        )
        
        self.logger.info(f"Dataset split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        # ìŠ¤ì¼€ì¼ë§ (ìˆ˜ì¹˜í˜• íŠ¹ì„±ë§Œ)
        from sklearn.preprocessing import StandardScaler
        
        numeric_features = X_train.select_dtypes(include=[np.number]).columns
        scaler = StandardScaler()
        
        X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
        X_val[numeric_features] = scaler.transform(X_val[numeric_features])
        X_test[numeric_features] = scaler.transform(X_test[numeric_features])
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        import joblib
        joblib.dump(scaler, 'models/scaler.pkl')
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)

# Feature Store êµ¬í˜„
class FeatureStore:
    def __init__(self, storage_backend='feast'):
        self.storage_backend = storage_backend
        
    def register_features(self, feature_df: pd.DataFrame, feature_group: str):
        """íŠ¹ì„± ì €ì¥ì†Œì— íŠ¹ì„± ë“±ë¡"""
        
        if self.storage_backend == 'feast':
            from feast import FeatureStore, Entity, FeatureView, FileSource
            
            # Feast ì„¤ì •
            fs = FeatureStore(repo_path="feature_repo")
            
            # ì—”í‹°í‹° ì •ì˜
            user_entity = Entity(
                name="user_id",
                value_type="INT64",
                description="User identifier"
            )
            
            # íŠ¹ì„± ë·° ì •ì˜
            user_features = FeatureView(
                name=feature_group,
                entities=["user_id"],
                ttl=timedelta(days=1),
                features=feature_df.columns.tolist(),
                online=True,
                source=FileSource(
                    path="data/user_features.parquet",
                    event_timestamp_column="timestamp"
                )
            )
            
            # íŠ¹ì„± ë“±ë¡
            fs.apply([user_entity, user_features])
            
        return f"Features registered in {feature_group}"
```

#### ìë™í™”ëœ EDA ë° íŠ¹ì„± ì„ íƒ
```python
# src/data/auto_eda.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pandas_profiling import ProfileReport
import shap
from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif

class AutomatedEDA:
    def __init__(self, df: pd.DataFrame, target_column: str):
        self.df = df
        self.target_column = target_column
        self.numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_features = df.select_dtypes(include=['object']).columns.tolist()
        
    def generate_eda_report(self, output_path: str = 'reports/eda_report.html'):
        """í¬ê´„ì ì¸ EDA ë¦¬í¬íŠ¸ ìƒì„±"""
        
        profile = ProfileReport(
            self.df,
            title='Automated EDA Report',
            explorative=True,
            interactions={"continuous": True},
            missing_diagrams={"heatmap": True, "dendrogram": True}
        )
        
        profile.to_file(output_path)
        
        return output_path
    
    def analyze_target_distribution(self):
        """íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„"""
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # ë¶„í¬ í”Œë¡¯
        if self.target_column in self.numeric_features:
            self.df[self.target_column].hist(ax=axes[0], bins=50)
            axes[0].set_title('Target Distribution')
            
            # ë¡œê·¸ ë³€í™˜ ë¶„í¬
            if (self.df[self.target_column] > 0).all():
                np.log1p(self.df[self.target_column]).hist(ax=axes[1], bins=50)
                axes[1].set_title('Log-transformed Target Distribution')
        else:
            # ë¶„ë¥˜ ë¬¸ì œ
            self.df[self.target_column].value_counts().plot(kind='bar', ax=axes[0])
            axes[0].set_title('Target Class Distribution')
            
            # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨
            class_ratios = self.df[self.target_column].value_counts(normalize=True)
            class_ratios.plot(kind='pie', ax=axes[1], autopct='%1.1f%%')
            axes[1].set_title('Class Balance')
        
        plt.tight_layout()
        plt.savefig('reports/target_analysis.png')
        
        return self.df[self.target_column].describe()
    
    def feature_importance_analysis(self, X, y, method='all'):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        
        importance_scores = {}
        
        # 1. ìƒê´€ê´€ê³„ ë¶„ì„ (ìˆ˜ì¹˜í˜• íƒ€ê²Ÿ)
        if self.target_column in self.numeric_features:
            correlations = X.corrwith(y).abs().sort_values(ascending=False)
            importance_scores['correlation'] = correlations
        
        # 2. Mutual Information
        mi_scores = mutual_info_classif(X, y) if y.dtype == 'object' else mutual_info_classif(X, y)
        importance_scores['mutual_info'] = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)
        
        # 3. Chi-squared (ë²”ì£¼í˜• íƒ€ê²Ÿ)
        if y.dtype == 'object':
            chi_scores = SelectKBest(chi2, k='all').fit(X, y).scores_
            importance_scores['chi2'] = pd.Series(chi_scores, index=X.columns).sort_values(ascending=False)
        
        # 4. Random Forest íŠ¹ì„± ì¤‘ìš”ë„
        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
        
        if y.dtype == 'object':
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
        else:
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
        
        rf.fit(X, y)
        importance_scores['random_forest'] = pd.Series(
            rf.feature_importances_, 
            index=X.columns
        ).sort_values(ascending=False)
        
        # 5. SHAP ê°’ (ìƒ˜í”Œë§ìœ¼ë¡œ ì†ë„ ê°œì„ )
        sample_size = min(1000, len(X))
        X_sample = X.sample(n=sample_size, random_state=42)
        
        explainer = shap.TreeExplainer(rf)
        shap_values = explainer.shap_values(X_sample)
        
        if isinstance(shap_values, list):  # ë‹¤ì¤‘ í´ë˜ìŠ¤
            shap_importance = np.abs(shap_values).mean(0).mean(0)
        else:
            shap_importance = np.abs(shap_values).mean(0)
            
        importance_scores['shap'] = pd.Series(
            shap_importance, 
            index=X.columns
        ).sort_values(ascending=False)
        
        # ì¢…í•© ìˆœìœ„
        combined_ranks = pd.DataFrame(importance_scores)
        combined_ranks['mean_rank'] = combined_ranks.rank(ascending=False).mean(axis=1)
        combined_ranks = combined_ranks.sort_values('mean_rank')
        
        # ì‹œê°í™”
        fig, ax = plt.subplots(figsize=(10, 8))
        combined_ranks.head(20).plot(kind='barh', ax=ax)
        ax.set_title('Feature Importance Scores (Top 20)')
        plt.tight_layout()
        plt.savefig('reports/feature_importance.png')
        
        return combined_ranks
    
    def recommend_features(self, importance_df: pd.DataFrame, threshold: float = 0.95):
        """íŠ¹ì„± ì„ íƒ ì¶”ì²œ"""
        
        # ëˆ„ì  ì¤‘ìš”ë„ ê³„ì‚°
        cumsum_importance = importance_df['random_forest'].cumsum() / importance_df['random_forest'].sum()
        
        # ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ìµœì†Œ íŠ¹ì„± ì§‘í•©
        selected_features = cumsum_importance[cumsum_importance <= threshold].index.tolist()
        
        return {
            'selected_features': selected_features,
            'n_features': len(selected_features),
            'cumulative_importance': cumsum_importance[selected_features[-1]]
        }
```

---

### ğŸ¤– 3ë‹¨ê³„: ëª¨ë¸ ê°œë°œ ë° ì‹¤í—˜ (1ì‹œê°„ 30ë¶„)

ë‹¤ìŒ ML ëª¨ë¸ ê°œë°œ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### AutoML íŒŒì´í”„ë¼ì¸
```python
# src/models/automl_pipeline.py
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple
import optuna
from sklearn.model_selection import cross_val_score
import mlflow
import mlflow.sklearn
from datetime import datetime

class AutoMLPipeline:
    def __init__(self, task_type: str, config: Dict[str, Any]):
        self.task_type = task_type  # classification, regression
        self.config = config
        self.models = self._get_model_candidates()
        self.best_model = None
        self.best_params = None
        self.best_score = None
        
        # MLflow ì„¤ì •
        mlflow.set_tracking_uri(config.get('mlflow_uri', 'http://localhost:5000'))
        mlflow.set_experiment(config.get('experiment_name', 'automl_experiment'))
    
    def _get_model_candidates(self) -> Dict[str, Any]:
        """íƒœìŠ¤í¬ì— ë”°ë¥¸ ëª¨ë¸ í›„ë³´êµ° ì •ì˜"""
        
        if self.task_type == 'classification':
            from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
            from sklearn.linear_model import LogisticRegression
            from sklearn.svm import SVC
            from xgboost import XGBClassifier
            from lightgbm import LGBMClassifier
            
            return {
                'logistic_regression': {
                    'model': LogisticRegression,
                    'params': {
                        'C': ('float', 0.01, 100, 'log'),
                        'penalty': ('categorical', ['l1', 'l2']),
                        'solver': ('categorical', ['liblinear', 'saga'])
                    }
                },
                'random_forest': {
                    'model': RandomForestClassifier,
                    'params': {
                        'n_estimators': ('int', 50, 500),
                        'max_depth': ('int', 3, 20),
                        'min_samples_split': ('int', 2, 20),
                        'min_samples_leaf': ('int', 1, 10)
                    }
                },
                'xgboost': {
                    'model': XGBClassifier,
                    'params': {
                        'n_estimators': ('int', 50, 500),
                        'max_depth': ('int', 3, 10),
                        'learning_rate': ('float', 0.01, 0.3, 'log'),
                        'subsample': ('float', 0.6, 1.0),
                        'colsample_bytree': ('float', 0.6, 1.0)
                    }
                },
                'lightgbm': {
                    'model': LGBMClassifier,
                    'params': {
                        'n_estimators': ('int', 50, 500),
                        'num_leaves': ('int', 20, 300),
                        'learning_rate': ('float', 0.01, 0.3, 'log'),
                        'feature_fraction': ('float', 0.6, 1.0),
                        'bagging_fraction': ('float', 0.6, 1.0)
                    }
                }
            }
        else:  # regression
            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
            from sklearn.linear_model import LinearRegression, Ridge, Lasso
            from xgboost import XGBRegressor
            from lightgbm import LGBMRegressor
            
            return {
                'linear_regression': {
                    'model': Ridge,
                    'params': {
                        'alpha': ('float', 0.01, 100, 'log')
                    }
                },
                'random_forest': {
                    'model': RandomForestRegressor,
                    'params': {
                        'n_estimators': ('int', 50, 500),
                        'max_depth': ('int', 3, 20),
                        'min_samples_split': ('int', 2, 20)
                    }
                },
                'xgboost': {
                    'model': XGBRegressor,
                    'params': {
                        'n_estimators': ('int', 50, 500),
                        'max_depth': ('int', 3, 10),
                        'learning_rate': ('float', 0.01, 0.3, 'log'),
                        'subsample': ('float', 0.6, 1.0)
                    }
                },
                'lightgbm': {
                    'model': LGBMRegressor,
                    'params': {
                        'n_estimators': ('int', 50, 500),
                        'num_leaves': ('int', 20, 300),
                        'learning_rate': ('float', 0.01, 0.3, 'log')
                    }
                }
            }
    
    def optimize_model(self, model_name: str, X_train, y_train, X_val, y_val, n_trials: int = 50):
        """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        
        model_config = self.models[model_name]
        
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
            params = {}
            for param_name, param_config in model_config['params'].items():
                if param_config[0] == 'int':
                    params[param_name] = trial.suggest_int(param_name, param_config[1], param_config[2])
                elif param_config[0] == 'float':
                    if len(param_config) > 3 and param_config[3] == 'log':
                        params[param_name] = trial.suggest_float(param_name, param_config[1], param_config[2], log=True)
                    else:
                        params[param_name] = trial.suggest_float(param_name, param_config[1], param_config[2])
                elif param_config[0] == 'categorical':
                    params[param_name] = trial.suggest_categorical(param_name, param_config[1])
            
            # ëª¨ë¸ í•™ìŠµ
            model = model_config['model'](**params)
            model.fit(X_train, y_train)
            
            # ê²€ì¦ ì ìˆ˜
            if self.task_type == 'classification':
                from sklearn.metrics import roc_auc_score
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                score = roc_auc_score(y_val, y_pred_proba)
            else:
                from sklearn.metrics import mean_squared_error
                y_pred = model.predict(X_val)
                score = -mean_squared_error(y_val, y_pred)  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜
            
            return score
        
        # Optuna ìŠ¤í„°ë”” ìƒì„± ë° ìµœì í™”
        study = optuna.create_study(
            direction='maximize',
            study_name=f'{model_name}_optimization'
        )
        
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        best_model = model_config['model'](**best_params)
        best_model.fit(X_train, y_train)
        
        return best_model, best_params, study.best_value
    
    def run_automl(self, X_train, y_train, X_val, y_val, time_budget_minutes: int = 60):
        """AutoML ì‹¤í–‰"""
        
        import time
        start_time = time.time()
        results = {}
        
        with mlflow.start_run(run_name=f"automl_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            
            for model_name in self.models.keys():
                if time.time() - start_time > time_budget_minutes * 60:
                    break
                
                print(f"\nOptimizing {model_name}...")
                
                # ê° ëª¨ë¸ì— ëŒ€í•œ ì¤‘ì²© ì‹¤í–‰
                with mlflow.start_run(run_name=model_name, nested=True):
                    
                    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
                    model, params, score = self.optimize_model(
                        model_name, X_train, y_train, X_val, y_val
                    )
                    
                    # ê²°ê³¼ ì €ì¥
                    results[model_name] = {
                        'model': model,
                        'params': params,
                        'score': score
                    }
                    
                    # MLflow ë¡œê¹…
                    mlflow.log_params(params)
                    mlflow.log_metric('validation_score', score)
                    mlflow.sklearn.log_model(model, model_name)
                    
                    # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹…
                    self._log_additional_metrics(model, X_val, y_val)
            
            # ìµœê³  ëª¨ë¸ ì„ íƒ
            best_model_name = max(results, key=lambda x: results[x]['score'])
            self.best_model = results[best_model_name]['model']
            self.best_params = results[best_model_name]['params']
            self.best_score = results[best_model_name]['score']
            
            # ìµœê³  ëª¨ë¸ íƒœê·¸
            mlflow.set_tag('best_model', best_model_name)
            mlflow.log_metric('best_score', self.best_score)
        
        return results
    
    def _log_additional_metrics(self, model, X_val, y_val):
        """ì¶”ê°€ í‰ê°€ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        
        y_pred = model.predict(X_val)
        
        if self.task_type == 'classification':
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            mlflow.log_metric('accuracy', accuracy_score(y_val, y_pred))
            mlflow.log_metric('precision', precision_score(y_val, y_pred, average='weighted'))
            mlflow.log_metric('recall', recall_score(y_val, y_pred, average='weighted'))
            mlflow.log_metric('f1_score', f1_score(y_val, y_pred, average='weighted'))
            
            # Confusion Matrix
            from sklearn.metrics import confusion_matrix
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            cm = confusion_matrix(y_val, y_pred)
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title('Confusion Matrix')
            plt.savefig('confusion_matrix.png')
            mlflow.log_artifact('confusion_matrix.png')
            
        else:  # regression
            from sklearn.metrics import mean_absolute_error, r2_score
            
            mlflow.log_metric('mae', mean_absolute_error(y_val, y_pred))
            mlflow.log_metric('rmse', np.sqrt(mean_squared_error(y_val, y_pred)))
            mlflow.log_metric('r2_score', r2_score(y_val, y_pred))
            
            # Residual Plot
            plt.figure(figsize=(10, 6))
            plt.scatter(y_pred, y_val - y_pred, alpha=0.5)
            plt.axhline(y=0, color='r', linestyle='--')
            plt.xlabel('Predicted Values')
            plt.ylabel('Residuals')
            plt.title('Residual Plot')
            plt.savefig('residual_plot.png')
            mlflow.log_artifact('residual_plot.png')

# ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ (ì„ íƒì )
class DeepLearningPipeline:
    def __init__(self, model_type: str, config: Dict[str, Any]):
        self.model_type = model_type
        self.config = config
        
    def build_neural_network(self, input_shape: int, output_shape: int):
        """ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ êµ¬ì¶•"""
        
        import tensorflow as tf
        from tensorflow.keras import layers, models
        
        if self.model_type == 'classification':
            model = models.Sequential([
                layers.Dense(128, activation='relu', input_shape=(input_shape,)),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                
                layers.Dense(64, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                
                layers.Dense(32, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.2),
                
                layers.Dense(output_shape, activation='softmax' if output_shape > 1 else 'sigmoid')
            ])
            
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='sparse_categorical_crossentropy' if output_shape > 1 else 'binary_crossentropy',
                metrics=['accuracy']
            )
            
        elif self.model_type == 'regression':
            model = models.Sequential([
                layers.Dense(128, activation='relu', input_shape=(input_shape,)),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                
                layers.Dense(64, activation='relu'),
                layers.BatchNormalization(),
                layers.Dropout(0.3),
                
                layers.Dense(32, activation='relu'),
                layers.BatchNormalization(),
                
                layers.Dense(1)  # íšŒê·€ëŠ” ì¶œë ¥ì´ 1ê°œ
            ])
            
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                loss='mse',
                metrics=['mae']
            )
            
        elif self.model_type == 'nlp':
            # Transformer ê¸°ë°˜ ëª¨ë¸
            from transformers import TFAutoModelForSequenceClassification
            
            model = TFAutoModelForSequenceClassification.from_pretrained(
                'bert-base-uncased',
                num_labels=output_shape
            )
            
        elif self.model_type == 'computer_vision':
            # CNN ì•„í‚¤í…ì²˜
            model = models.Sequential([
                layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
                layers.MaxPooling2D((2, 2)),
                
                layers.Conv2D(64, (3, 3), activation='relu'),
                layers.MaxPooling2D((2, 2)),
                
                layers.Conv2D(128, (3, 3), activation='relu'),
                layers.MaxPooling2D((2, 2)),
                
                layers.Flatten(),
                layers.Dense(128, activation='relu'),
                layers.Dropout(0.5),
                layers.Dense(output_shape, activation='softmax')
            ])
            
            model.compile(
                optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
        
        return model
    
    def train_with_callbacks(self, model, X_train, y_train, X_val, y_val):
        """ì½œë°±ì„ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ"""
        
        import tensorflow as tf
        
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6
            ),
            tf.keras.callbacks.ModelCheckpoint(
                'best_model.h5',
                monitor='val_loss',
                save_best_only=True
            ),
            tf.keras.callbacks.TensorBoard(
                log_dir='./logs',
                histogram_freq=1
            )
        ]
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        return model, history
```

#### ëª¨ë¸ í‰ê°€ ë° í•´ì„
```python
# src/models/model_evaluation.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, roc_curve, auc
import shap
import lime
import lime.lime_tabular

class ModelEvaluator:
    def __init__(self, model, model_type: str):
        self.model = model
        self.model_type = model_type
        
    def comprehensive_evaluation(self, X_test, y_test, feature_names=None):
        """í¬ê´„ì ì¸ ëª¨ë¸ í‰ê°€"""
        
        evaluation_results = {}
        
        # 1. ê¸°ë³¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        y_pred = self.model.predict(X_test)
        
        if self.model_type == 'classification':
            # ë¶„ë¥˜ ë¦¬í¬íŠ¸
            report = classification_report(y_test, y_pred, output_dict=True)
            evaluation_results['classification_report'] = report
            
            # ROC ì»¤ë¸Œ (ì´ì§„ ë¶„ë¥˜)
            if len(np.unique(y_test)) == 2:
                y_pred_proba = self.model.predict_proba(X_test)[:, 1]
                fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
                roc_auc = auc(fpr, tpr)
                
                plt.figure(figsize=(8, 6))
                plt.plot(fpr, tpr, color='darkorange', lw=2, 
                        label=f'ROC curve (AUC = {roc_auc:.2f})')
                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
                plt.xlim([0.0, 1.0])
                plt.ylim([0.0, 1.05])
                plt.xlabel('False Positive Rate')
                plt.ylabel('True Positive Rate')
                plt.title('ROC Curve')
                plt.legend(loc="lower right")
                plt.savefig('reports/roc_curve.png')
                
                evaluation_results['roc_auc'] = roc_auc
                
        else:  # regression
            from sklearn.metrics import mean_absolute_error, r2_score
            
            evaluation_results['mae'] = mean_absolute_error(y_test, y_pred)
            evaluation_results['rmse'] = np.sqrt(np.mean((y_test - y_pred) ** 2))
            evaluation_results['r2'] = r2_score(y_test, y_pred)
            
            # ì˜ˆì¸¡ vs ì‹¤ì œ í”Œë¡¯
            plt.figure(figsize=(8, 6))
            plt.scatter(y_test, y_pred, alpha=0.5)
            plt.plot([y_test.min(), y_test.max()], 
                    [y_test.min(), y_test.max()], 'r--', lw=2)
            plt.xlabel('Actual Values')
            plt.ylabel('Predicted Values')
            plt.title('Predicted vs Actual')
            plt.savefig('reports/predicted_vs_actual.png')
        
        # 2. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        if hasattr(self.model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': feature_names or [f'feature_{i}' for i in range(X_test.shape[1])],
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            plt.figure(figsize=(10, 8))
            feature_importance.head(20).plot(x='feature', y='importance', kind='barh')
            plt.title('Top 20 Feature Importances')
            plt.tight_layout()
            plt.savefig('reports/feature_importance_model.png')
            
            evaluation_results['feature_importance'] = feature_importance
        
        return evaluation_results
    
    def explain_predictions(self, X_test, sample_indices=[0, 1, 2], feature_names=None):
        """ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…"""
        
        explanations = {}
        
        # SHAP ì„¤ëª…
        if hasattr(self.model, 'predict'):
            # TreeExplainer for tree-based models
            try:
                explainer = shap.TreeExplainer(self.model)
                shap_values = explainer.shap_values(X_test)
                
                # SHAP Summary Plot
                plt.figure()
                shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)
                plt.savefig('reports/shap_summary.png')
                plt.close()
                
                # Individual explanations
                for idx in sample_indices:
                    plt.figure()
                    shap.waterfall_plot(
                        shap.Explanation(
                            values=shap_values[idx] if len(shap_values.shape) == 2 else shap_values[0][idx],
                            base_values=explainer.expected_value if isinstance(explainer.expected_value, float) else explainer.expected_value[0],
                            data=X_test[idx],
                            feature_names=feature_names
                        ),
                        show=False
                    )
                    plt.savefig(f'reports/shap_waterfall_sample_{idx}.png')
                    plt.close()
                    
            except:
                # KernelExplainer for black-box models
                explainer = shap.KernelExplainer(self.model.predict, X_test[:100])
                shap_values = explainer.shap_values(X_test[sample_indices])
        
        # LIME ì„¤ëª…
        explainer_lime = lime.lime_tabular.LimeTabularExplainer(
            X_test,
            feature_names=feature_names,
            mode='classification' if self.model_type == 'classification' else 'regression'
        )
        
        for idx in sample_indices:
            exp = explainer_lime.explain_instance(
                X_test[idx], 
                self.model.predict_proba if self.model_type == 'classification' else self.model.predict,
                num_features=10
            )
            
            # LIME í”Œë¡¯ ì €ì¥
            fig = exp.as_pyplot_figure()
            fig.savefig(f'reports/lime_explanation_sample_{idx}.png')
            plt.close()
            
            explanations[f'sample_{idx}'] = exp.as_list()
        
        return explanations
    
    def fairness_evaluation(self, X_test, y_test, sensitive_features):
        """ê³µì •ì„± í‰ê°€"""
        
        from fairlearn.metrics import MetricFrame
        from sklearn.metrics import accuracy_score, precision_score, recall_score
        
        y_pred = self.model.predict(X_test)
        
        # ë¯¼ê°í•œ íŠ¹ì„±ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        metrics = {
            'accuracy': accuracy_score,
            'precision': lambda y_t, y_p: precision_score(y_t, y_p, average='weighted'),
            'recall': lambda y_t, y_p: recall_score(y_t, y_p, average='weighted')
        }
        
        fairness_results = {}
        
        for feature in sensitive_features:
            mf = MetricFrame(
                metrics=metrics,
                y_true=y_test,
                y_pred=y_pred,
                sensitive_features=X_test[feature]
            )
            
            # ê·¸ë£¹ë³„ ì„±ëŠ¥ ì°¨ì´
            fairness_results[feature] = {
                'overall': mf.overall.to_dict(),
                'by_group': mf.by_group.to_dict(),
                'difference': mf.difference().to_dict(),
                'ratio': mf.ratio().to_dict()
            }
            
            # ì‹œê°í™”
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))
            
            for idx, (metric_name, metric_values) in enumerate(mf.by_group.items()):
                metric_values.plot(kind='bar', ax=axes[idx])
                axes[idx].set_title(f'{metric_name} by {feature}')
                axes[idx].set_ylabel(metric_name)
            
            plt.tight_layout()
            plt.savefig(f'reports/fairness_{feature}.png')
        
        return fairness_results
```

---

### ğŸš€ 4ë‹¨ê³„: ëª¨ë¸ ë°°í¬ ì¤€ë¹„ (45ë¶„)

ë‹¤ìŒ ëª¨ë¸ ë°°í¬ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### ëª¨ë¸ ì„œë¹™ API
```python
# src/serving/model_api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import numpy as np
import pandas as pd
import joblib
import logging
from typing import List, Dict, Any, Optional
import redis
import json
from prometheus_client import Counter, Histogram, generate_latest
import time

# Prometheus ë©”íŠ¸ë¦­
prediction_counter = Counter('model_predictions_total', 'Total number of predictions')
prediction_duration = Histogram('model_prediction_duration_seconds', 'Prediction duration')
prediction_errors = Counter('model_prediction_errors_total', 'Total prediction errors')

app = FastAPI(title="ML Model API", version="1.0.0")

# ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë“œ
MODEL_PATH = "models/best_model.pkl"
SCALER_PATH = "models/scaler.pkl"
FEATURE_NAMES_PATH = "models/feature_names.json"

model = joblib.load(MODEL_PATH)
scaler = joblib.load(SCALER_PATH)
with open(FEATURE_NAMES_PATH, 'r') as f:
    feature_names = json.load(f)

# Redis ìºì‹œ ì„¤ì •
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PredictionRequest(BaseModel):
    """ì˜ˆì¸¡ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    features: Dict[str, Any] = Field(..., example={
        "age": 35,
        "income": 75000,
        "credit_score": 720,
        "account_age_days": 365
    })
    request_id: Optional[str] = Field(None, description="Unique request ID for tracking")

class PredictionResponse(BaseModel):
    """ì˜ˆì¸¡ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    prediction: float
    probability: Optional[Dict[str, float]] = None
    confidence: float
    model_version: str
    request_id: Optional[str]
    processing_time_ms: float

class BatchPredictionRequest(BaseModel):
    """ë°°ì¹˜ ì˜ˆì¸¡ ìš”ì²­"""
    instances: List[Dict[str, Any]]
    
@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "model_version": "1.0.0"
    }

@app.get("/metrics")
async def metrics():
    """Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸"""
    return generate_latest()

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """ë‹¨ì¼ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸"""
    
    start_time = time.time()
    prediction_counter.inc()
    
    try:
        # ìºì‹œ í™•ì¸
        cache_key = f"prediction:{hash(str(sorted(request.features.items())))}"
        cached_result = redis_client.get(cache_key)
        
        if cached_result:
            return json.loads(cached_result)
        
        # íŠ¹ì„± ì¤€ë¹„
        feature_vector = prepare_features(request.features)
        
        # ì˜ˆì¸¡
        with prediction_duration.time():
            prediction = model.predict(feature_vector)[0]
            
            # í™•ë¥  ê³„ì‚° (ë¶„ë¥˜ ëª¨ë¸ì¸ ê²½ìš°)
            probability = None
            confidence = 1.0
            
            if hasattr(model, 'predict_proba'):
                proba = model.predict_proba(feature_vector)[0]
                probability = {f"class_{i}": float(p) for i, p in enumerate(proba)}
                confidence = float(max(proba))
        
        # ì‘ë‹µ ìƒì„±
        response = PredictionResponse(
            prediction=float(prediction),
            probability=probability,
            confidence=confidence,
            model_version="1.0.0",
            request_id=request.request_id,
            processing_time_ms=(time.time() - start_time) * 1000
        )
        
        # ìºì‹œ ì €ì¥ (5ë¶„ TTL)
        redis_client.setex(cache_key, 300, response.json())
        
        # ë¡œê¹…
        logger.info(f"Prediction completed: {response.dict()}")
        
        return response
        
    except Exception as e:
        prediction_errors.inc()
        logger.error(f"Prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict_batch")
async def predict_batch(request: BatchPredictionRequest):
    """ë°°ì¹˜ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸"""
    
    start_time = time.time()
    results = []
    
    for instance in request.instances:
        try:
            feature_vector = prepare_features(instance)
            prediction = model.predict(feature_vector)[0]
            
            results.append({
                "prediction": float(prediction),
                "status": "success"
            })
        except Exception as e:
            results.append({
                "prediction": None,
                "status": "error",
                "error": str(e)
            })
    
    return {
        "predictions": results,
        "processing_time_ms": (time.time() - start_time) * 1000
    }

def prepare_features(raw_features: Dict[str, Any]) -> np.ndarray:
    """ì›ì‹œ íŠ¹ì„±ì„ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜"""
    
    # íŠ¹ì„± ìˆœì„œ ë§ì¶”ê¸°
    feature_vector = []
    for feature_name in feature_names:
        if feature_name not in raw_features:
            raise ValueError(f"Missing required feature: {feature_name}")
        feature_vector.append(raw_features[feature_name])
    
    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    feature_array = np.array(feature_vector).reshape(1, -1)
    
    # ìŠ¤ì¼€ì¼ë§ ì ìš©
    feature_array = scaler.transform(feature_array)
    
    return feature_array

# A/B í…ŒìŠ¤íŠ¸ ì§€ì›
class ABTestConfig:
    def __init__(self):
        self.models = {
            'control': joblib.load('models/model_v1.pkl'),
            'treatment': joblib.load('models/model_v2.pkl')
        }
        self.traffic_split = {'control': 0.5, 'treatment': 0.5}
    
    def get_model_variant(self, user_id: str):
        """ì‚¬ìš©ì ID ê¸°ë°˜ ëª¨ë¸ ë³€í˜• ì„ íƒ"""
        hash_value = hash(user_id) % 100
        if hash_value < self.traffic_split['control'] * 100:
            return 'control'
        return 'treatment'

# ëª¨ë¸ ëª¨ë‹ˆí„°ë§
class ModelMonitor:
    def __init__(self):
        self.prediction_log = []
        
    def log_prediction(self, features, prediction, actual=None):
        """ì˜ˆì¸¡ ë¡œê¹…"""
        self.prediction_log.append({
            'timestamp': time.time(),
            'features': features,
            'prediction': prediction,
            'actual': actual
        })
        
        # ë“œë¦¬í”„íŠ¸ ê°ì§€
        if len(self.prediction_log) % 1000 == 0:
            self.check_drift()
    
    def check_drift(self):
        """ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        recent_predictions = self.prediction_log[-1000:]
        # ë“œë¦¬í”„íŠ¸ ê°ì§€ ë¡œì§ êµ¬í˜„
        pass
```

#### ì»¨í…Œì´ë„ˆí™” ë° ë°°í¬ ì„¤ì •
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„±
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„±
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY src/ ./src/
COPY models/ ./models/

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# ì‹¤í–‰
CMD ["uvicorn", "src.serving.model_api:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# kubernetes/model-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-api
  labels:
    app: ml-model
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model
  template:
    metadata:
      labels:
        app: ml-model
    spec:
      containers:
      - name: model-api
        image: ml-model:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        env:
        - name: MODEL_VERSION
          value: "1.0.0"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ml-model-service
spec:
  selector:
    app: ml-model
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-model-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-model-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

### ğŸ“Š 5ë‹¨ê³„: MLOps íŒŒì´í”„ë¼ì¸ ë° ëª¨ë‹ˆí„°ë§ (30ë¶„)

ë‹¤ìŒ MLOps ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ì¤˜:

#### CI/CD íŒŒì´í”„ë¼ì¸
```yaml
# .github/workflows/ml-pipeline.yml
name: ML Model Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      run: |
        pytest tests/ --cov=src --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v1
      with:
        file: ./coverage.xml

  train:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: pip install -r requirements.txt
    
    - name: Download data
      run: |
        aws s3 cp s3://ml-data/train_data.parquet data/
    
    - name: Train model
      run: |
        python src/train.py --config config/model_config.yaml
    
    - name: Evaluate model
      run: |
        python src/evaluate.py --model models/latest_model.pkl
    
    - name: Upload model to registry
      if: success()
      run: |
        python src/register_model.py --model models/latest_model.pkl

  deploy:
    needs: train
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Build Docker image
      run: |
        docker build -t ml-model:${{ github.sha }} .
    
    - name: Push to registry
      run: |
        docker tag ml-model:${{ github.sha }} ${{ secrets.DOCKER_REGISTRY }}/ml-model:latest
        docker push ${{ secrets.DOCKER_REGISTRY }}/ml-model:latest
    
    - name: Deploy to Kubernetes
      run: |
        kubectl set image deployment/ml-model-api model-api=${{ secrets.DOCKER_REGISTRY }}/ml-model:latest
        kubectl rollout status deployment/ml-model-api

  monitor:
    needs: deploy
    runs-on: ubuntu-latest
    
    steps:
    - name: Run smoke tests
      run: |
        python tests/smoke_tests.py --endpoint https://api.example.com/predict
    
    - name: Check model metrics
      run: |
        python src/monitoring/check_metrics.py --threshold config/alert_thresholds.yaml
```

#### ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
```python
# src/monitoring/model_monitor.py
import pandas as pd
import numpy as np
from typing import Dict, Any, List
from datetime import datetime, timedelta
import requests
from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, TargetDriftPreset
from prometheus_client import Gauge, Counter
import logging

# Prometheus ë©”íŠ¸ë¦­
model_accuracy_gauge = Gauge('model_accuracy', 'Current model accuracy')
drift_score_gauge = Gauge('data_drift_score', 'Data drift score')
prediction_latency_gauge = Gauge('prediction_latency_ms', 'Prediction latency in milliseconds')
daily_predictions_counter = Counter('daily_predictions_total', 'Total daily predictions')

class ModelMonitor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.reference_data = pd.read_parquet(config['reference_data_path'])
        
    def monitor_performance(self, production_data: pd.DataFrame):
        """ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        
        # ì‹¤ì œ ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©
        labeled_data = production_data[production_data['actual_label'].notna()]
        
        if len(labeled_data) > 0:
            # ì •í™•ë„ ê³„ì‚°
            accuracy = (labeled_data['prediction'] == labeled_data['actual_label']).mean()
            model_accuracy_gauge.set(accuracy)
            
            # ì„±ëŠ¥ ì €í•˜ ì•Œë¦¼
            if accuracy < self.config['accuracy_threshold']:
                self._send_alert({
                    'type': 'performance_degradation',
                    'metric': 'accuracy',
                    'current_value': accuracy,
                    'threshold': self.config['accuracy_threshold']
                })
        
        return accuracy
    
    def detect_data_drift(self, current_data: pd.DataFrame):
        """ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        
        # Evidently ë¦¬í¬íŠ¸ ìƒì„±
        data_drift_report = Report(metrics=[
            DataDriftPreset(),
        ])
        
        data_drift_report.run(
            reference_data=self.reference_data,
            current_data=current_data
        )
        
        # ë“œë¦¬í”„íŠ¸ ì ìˆ˜ ì¶”ì¶œ
        drift_results = data_drift_report.as_dict()
        drift_score = drift_results['metrics'][0]['result']['drift_score']
        
        drift_score_gauge.set(drift_score)
        
        # ë“œë¦¬í”„íŠ¸ ì•Œë¦¼
        if drift_score > self.config['drift_threshold']:
            self._send_alert({
                'type': 'data_drift',
                'drift_score': drift_score,
                'threshold': self.config['drift_threshold'],
                'drifted_features': self._get_drifted_features(drift_results)
            })
        
        return drift_results
    
    def monitor_prediction_distribution(self, predictions: List[float]):
        """ì˜ˆì¸¡ ë¶„í¬ ëª¨ë‹ˆí„°ë§"""
        
        pred_array = np.array(predictions)
        
        # ë¶„í¬ í†µê³„
        stats = {
            'mean': np.mean(pred_array),
            'std': np.std(pred_array),
            'min': np.min(pred_array),
            'max': np.max(pred_array),
            'percentiles': {
                '25': np.percentile(pred_array, 25),
                '50': np.percentile(pred_array, 50),
                '75': np.percentile(pred_array, 75)
            }
        }
        
        # ì´ìƒ íŒ¨í„´ ê°ì§€
        if stats['std'] < self.config['min_prediction_std']:
            self._send_alert({
                'type': 'prediction_pattern_anomaly',
                'issue': 'low_variance',
                'std': stats['std']
            })
        
        return stats
    
    def _send_alert(self, alert_data: Dict[str, Any]):
        """ì•Œë¦¼ ì „ì†¡"""
        
        alert_data['timestamp'] = datetime.now().isoformat()
        alert_data['model_version'] = self.config['model_version']
        
        # Slack ì•Œë¦¼
        if self.config.get('slack_webhook'):
            requests.post(
                self.config['slack_webhook'],
                json={
                    'text': f"ğŸš¨ Model Alert: {alert_data['type']}",
                    'blocks': [
                        {
                            'type': 'section',
                            'text': {
                                'type': 'mrkdwn',
                                'text': f"*Alert Type:* {alert_data['type']}\n"
                                       f"*Details:* ```{alert_data}```"
                            }
                        }
                    ]
                }
            )
        
        # ë¡œê·¸ ê¸°ë¡
        self.logger.warning(f"Model alert: {alert_data}")

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
def create_monitoring_dashboard():
    """Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •"""
    
    dashboard_config = {
        "dashboard": {
            "title": "ML Model Monitoring",
            "panels": [
                {
                    "title": "Model Accuracy",
                    "targets": [{"expr": "model_accuracy"}],
                    "type": "graph"
                },
                {
                    "title": "Data Drift Score",
                    "targets": [{"expr": "data_drift_score"}],
                    "type": "gauge",
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {"color": "green", "value": 0},
                            {"color": "yellow", "value": 0.1},
                            {"color": "red", "value": 0.3}
                        ]
                    }
                },
                {
                    "title": "Prediction Latency",
                    "targets": [{"expr": "histogram_quantile(0.95, prediction_latency_ms)"}],
                    "type": "graph"
                },
                {
                    "title": "Daily Predictions",
                    "targets": [{"expr": "rate(daily_predictions_total[1d])"}],
                    "type": "stat"
                }
            ]
        }
    }
    
    return dashboard_config
```

---

### ğŸ“‹ 6ë‹¨ê³„: ë¬¸ì„œí™” ë° ëª¨ë¸ ê±°ë²„ë„ŒìŠ¤ (15ë¶„)

ë‹¤ìŒ ë¬¸ì„œì™€ ê±°ë²„ë„ŒìŠ¤ ì²´ê³„ë¥¼ ìƒì„±í•´ì¤˜:

#### ëª¨ë¸ ì¹´ë“œ ë° ë¬¸ì„œí™”
```markdown
# Model Card: ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸

## ëª¨ë¸ ê°œìš”
- **ëª¨ë¸ëª…**: Customer Churn Predictor v1.0
- **ê°œë°œì¼**: 2024-03-15
- **ê°œë°œíŒ€**: ML Engineering Team
- **ëª¨ë¸ ìœ í˜•**: Binary Classification (XGBoost)
- **í”„ë ˆì„ì›Œí¬**: scikit-learn 1.2.0, xgboost 1.7.0

## ì‚¬ìš© ëª©ì 
- **ì£¼ìš” ëª©ì **: ê³ ê° ì´íƒˆ ê°€ëŠ¥ì„± ì˜ˆì¸¡
- **ì‚¬ìš© ì‚¬ë¡€**: 
  - ê³ ìœ„í—˜ ê³ ê° ì‹ë³„
  - ë§ì¶¤í˜• ë¦¬í…ì…˜ ìº í˜ì¸
  - ê³ ê° ìƒì• ê°€ì¹˜ ìµœì í™”

## ë°ì´í„°
- **í•™ìŠµ ë°ì´í„°**: 
  - ê¸°ê°„: 2022-01-01 ~ 2023-12-31
  - í¬ê¸°: 1.2M ê³ ê° ë ˆì½”ë“œ
  - íŠ¹ì„±: 45ê°œ (ì¸êµ¬í†µê³„, í–‰ë™, ê±°ë˜)
- **ê²€ì¦ ë°ì´í„°**: 2024-01-01 ~ 2024-02-28

## ì„±ëŠ¥ ë©”íŠ¸ë¦­
| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| Accuracy | 0.92 |
| Precision | 0.89 |
| Recall | 0.85 |
| F1-Score | 0.87 |
| AUC-ROC | 0.94 |

## í•œê³„ ë° í¸í–¥
- **ì•Œë ¤ì§„ í•œê³„**:
  - ì‹ ê·œ ê³ ê° (< 30ì¼) ì˜ˆì¸¡ ì •í™•ë„ ë‚®ìŒ
  - ê³„ì ˆì„± íŒ¨í„´ ì™„ì „íˆ í¬ì°© ëª»í•¨
- **í¸í–¥ ë¶„ì„**:
  - ì—°ë ¹ëŒ€ë³„ ì„±ëŠ¥ ì°¨ì´ < 5%
  - ì§€ì—­ë³„ ì„±ëŠ¥ ê· ì¼

## ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­
- ê°œì¸ì •ë³´ ë³´í˜¸ ì¤€ìˆ˜ (GDPR, CCPA)
- ì„¤ëª… ê°€ëŠ¥í•œ AI ì›ì¹™ ì ìš©
- ê³µì •ì„± ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§

## ì‚¬ìš© ê°€ì´ë“œ
```python
# ì˜ˆì¸¡ API í˜¸ì¶œ
response = requests.post(
    "https://api.example.com/predict",
    json={
        "features": {
            "account_age_days": 365,
            "total_transactions": 42,
            "avg_transaction_value": 125.50
        }
    }
)
```

## ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜
- ì¼ì¼ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ì£¼ê°„ ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê²€ì‚¬
- ì›”ê°„ ì¬í•™ìŠµ í‰ê°€
```

#### ëª¨ë¸ ê±°ë²„ë„ŒìŠ¤ í”„ë¡œì„¸ìŠ¤
```python
# src/governance/model_registry.py
from typing import Dict, Any, Optional
import mlflow
from datetime import datetime
import json

class ModelRegistry:
    def __init__(self, registry_uri: str):
        self.registry_uri = registry_uri
        mlflow.set_registry_uri(registry_uri)
        
    def register_model(self, model_path: str, model_metadata: Dict[str, Any]):
        """ëª¨ë¸ ë“±ë¡ ë° ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤"""
        
        # ëª¨ë¸ ë“±ë¡
        model_uri = f"runs:/{model_metadata['run_id']}/model"
        model_version = mlflow.register_model(
            model_uri,
            model_metadata['model_name']
        )
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        client = mlflow.tracking.MlflowClient()
        
        # ëª¨ë¸ ë²„ì „ íƒœê·¸
        client.set_model_version_tag(
            model_metadata['model_name'],
            model_version.version,
            "developer",
            model_metadata['developer']
        )
        
        client.set_model_version_tag(
            model_metadata['model_name'],
            model_version.version,
            "performance_metrics",
            json.dumps(model_metadata['metrics'])
        )
        
        # ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš°
        self._approval_workflow(
            model_metadata['model_name'],
            model_version.version,
            model_metadata
        )
        
        return model_version
    
    def _approval_workflow(self, model_name: str, version: str, metadata: Dict[str, Any]):
        """ëª¨ë¸ ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš°"""
        
        client = mlflow.tracking.MlflowClient()
        
        # 1ë‹¨ê³„: ê°œë°œ í™˜ê²½
        client.transition_model_version_stage(
            name=model_name,
            version=version,
            stage="Staging"
        )
        
        # 2ë‹¨ê³„: ìë™ ê²€ì¦
        validation_passed = self._run_validation_checks(model_name, version, metadata)
        
        if validation_passed:
            # 3ë‹¨ê³„: í”„ë¡œë•ì…˜ ìŠ¹ì¸ ìš”ì²­
            client.set_model_version_tag(
                model_name,
                version,
                "approval_status",
                "pending_review"
            )
            
            # ìŠ¹ì¸ìì—ê²Œ ì•Œë¦¼
            self._notify_approvers(model_name, version, metadata)
        else:
            client.set_model_version_tag(
                model_name,
                version,
                "approval_status",
                "failed_validation"
            )
    
    def _run_validation_checks(self, model_name: str, version: str, metadata: Dict[str, Any]) -> bool:
        """ìë™ ê²€ì¦ ì²´í¬"""
        
        checks = {
            "performance_threshold": metadata['metrics']['accuracy'] > 0.85,
            "fairness_check": all(bias < 0.1 for bias in metadata.get('bias_metrics', {}).values()),
            "security_scan": self._security_scan_passed(model_name, version),
            "documentation_complete": all(k in metadata for k in ['description', 'limitations', 'ethics'])
        }
        
        return all(checks.values())
    
    def promote_to_production(self, model_name: str, version: str, approver: str):
        """í”„ë¡œë•ì…˜ ë°°í¬ ìŠ¹ì¸"""
        
        client = mlflow.tracking.MlflowClient()
        
        # í”„ë¡œë•ì…˜ìœ¼ë¡œ ì „í™˜
        client.transition_model_version_stage(
            name=model_name,
            version=version,
            stage="Production"
        )
        
        # ìŠ¹ì¸ ê¸°ë¡
        client.set_model_version_tag(
            model_name,
            version,
            "approved_by",
            approver
        )
        
        client.set_model_version_tag(
            model_name,
            version,
            "approval_timestamp",
            datetime.now().isoformat()
        )
        
        # ì´ì „ í”„ë¡œë•ì…˜ ëª¨ë¸ ì•„ì¹´ì´ë¸Œ
        self._archive_previous_production_models(model_name, version)
        
        return True
```

**ê²°ê³¼ë¬¼**:
- ì™„ì „í•œ ML/AI ê°œë°œ íŒŒì´í”„ë¼ì¸
- AutoML ë° ë”¥ëŸ¬ë‹ ì§€ì›
- ëª¨ë¸ ì„œë¹™ API ë° ì¸í”„ë¼
- MLOps CI/CD íŒŒì´í”„ë¼ì¸
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
- ëª¨ë¸ ê±°ë²„ë„ŒìŠ¤ ì²´ê³„

---

ì´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ë©´ ë°ì´í„° ì¤€ë¹„ë¶€í„° í”„ë¡œë•ì…˜ ë°°í¬ê¹Œì§€ ì™„ì „í•œ ML/AI ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ êµ¬ì¶•ë©ë‹ˆë‹¤!