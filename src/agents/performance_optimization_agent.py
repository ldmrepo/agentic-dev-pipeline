"""
ì„±ëŠ¥ ìµœì í™” ì „ë¬¸ ì—ì´ì „íŠ¸
ì½”ë“œ, ë°ì´í„°ë² ì´ìŠ¤, ì¸í”„ë¼ ì „ë°˜ì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ìµœì í™”í•˜ëŠ” AI ì—ì´ì „íŠ¸
"""

import asyncio
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import psutil
import aiohttp
import yaml
import json
import subprocess
from datetime import datetime
import pandas as pd
import numpy as np
from abc import ABC, abstractmethod

class PerformanceIssueType(Enum):
    """ì„±ëŠ¥ ì´ìŠˆ íƒ€ì…"""
    SLOW_QUERY = "slow_query"
    HIGH_CPU = "high_cpu"
    MEMORY_LEAK = "memory_leak"
    INEFFICIENT_ALGORITHM = "inefficient_algorithm"
    MISSING_INDEX = "missing_index"
    CACHE_MISS = "cache_miss"
    N_PLUS_ONE = "n_plus_one"
    BLOCKING_IO = "blocking_io"

@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    metric_name: str
    value: float
    unit: str
    timestamp: datetime
    context: Dict[str, Any]

@dataclass
class OptimizationRecommendation:
    """ìµœì í™” ê¶Œì¥ì‚¬í•­"""
    issue_type: PerformanceIssueType
    severity: str  # critical, high, medium, low
    description: str
    solution: str
    estimated_improvement: str
    implementation_code: Optional[str] = None
    risk_level: str = "low"

class PerformanceAnalyzer(ABC):
    """ì„±ëŠ¥ ë¶„ì„ê¸° ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def analyze(self, target: Any) -> List[PerformanceMetric]:
        """ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        pass
    
    @abstractmethod
    async def get_recommendations(self, metrics: List[PerformanceMetric]) -> List[OptimizationRecommendation]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        pass

class CodePerformanceAnalyzer(PerformanceAnalyzer):
    """ì½”ë“œ ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    async def analyze(self, code_path: str) -> List[PerformanceMetric]:
        """ì½”ë“œ í”„ë¡œíŒŒì¼ë§ ë° ë¶„ì„"""
        metrics = []
        
        # Python ì½”ë“œ í”„ë¡œíŒŒì¼ë§
        if code_path.endswith('.py'):
            profile_result = await self._profile_python(code_path)
            metrics.extend(profile_result)
        
        # JavaScript ì½”ë“œ ë¶„ì„
        elif code_path.endswith('.js') or code_path.endswith('.ts'):
            profile_result = await self._profile_javascript(code_path)
            metrics.extend(profile_result)
        
        # ë³µì¡ë„ ë¶„ì„
        complexity_metrics = await self._analyze_complexity(code_path)
        metrics.extend(complexity_metrics)
        
        return metrics
    
    async def _profile_python(self, file_path: str) -> List[PerformanceMetric]:
        """Python ì½”ë“œ í”„ë¡œíŒŒì¼ë§"""
        import cProfile
        import pstats
        from io import StringIO
        
        profiler = cProfile.Profile()
        metrics = []
        
        # í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
        profiler.enable()
        try:
            exec(open(file_path).read())
        except Exception as e:
            print(f"Profiling error: {e}")
        finally:
            profiler.disable()
        
        # ê²°ê³¼ ë¶„ì„
        s = StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats(10)  # ìƒìœ„ 10ê°œ í•¨ìˆ˜
        
        # ë©”íŠ¸ë¦­ ì¶”ì¶œ
        for line in s.getvalue().split('\n'):
            if line and not line.startswith(' '):
                parts = line.split()
                if len(parts) >= 6:
                    metrics.append(PerformanceMetric(
                        metric_name=f"function_time_{parts[-1]}",
                        value=float(parts[2]),
                        unit="seconds",
                        timestamp=datetime.now(),
                        context={"function": parts[-1], "calls": parts[0]}
                    ))
        
        return metrics
    
    async def _analyze_complexity(self, file_path: str) -> List[PerformanceMetric]:
        """ì½”ë“œ ë³µì¡ë„ ë¶„ì„"""
        # radonì„ ì‚¬ìš©í•œ ë³µì¡ë„ ë¶„ì„
        try:
            result = subprocess.run(
                ['radon', 'cc', file_path, '-j'],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                complexity_data = json.loads(result.stdout)
                metrics = []
                
                for file_data in complexity_data.values():
                    for func in file_data:
                        metrics.append(PerformanceMetric(
                            metric_name="cyclomatic_complexity",
                            value=func['complexity'],
                            unit="score",
                            timestamp=datetime.now(),
                            context={
                                "function": func['name'],
                                "type": func['type'],
                                "rank": func['rank']
                            }
                        ))
                
                return metrics
        except Exception as e:
            print(f"Complexity analysis error: {e}")
        
        return []
    
    async def get_recommendations(self, metrics: List[PerformanceMetric]) -> List[OptimizationRecommendation]:
        """ì½”ë“œ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ê³ ë³µì¡ë„ í•¨ìˆ˜ í™•ì¸
        for metric in metrics:
            if metric.metric_name == "cyclomatic_complexity" and metric.value > 10:
                recommendations.append(OptimizationRecommendation(
                    issue_type=PerformanceIssueType.INEFFICIENT_ALGORITHM,
                    severity="high" if metric.value > 20 else "medium",
                    description=f"í•¨ìˆ˜ '{metric.context['function']}'ì˜ ë³µì¡ë„ê°€ {metric.value}ë¡œ ë†’ìŠµë‹ˆë‹¤",
                    solution="í•¨ìˆ˜ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³  ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì„¸ìš”",
                    estimated_improvement="20-30% ì„±ëŠ¥ í–¥ìƒ",
                    implementation_code=self._generate_refactoring_code(metric.context['function'])
                ))
        
        # ëŠë¦° í•¨ìˆ˜ í™•ì¸
        slow_functions = [m for m in metrics if m.metric_name.startswith("function_time_") and m.value > 0.1]
        for metric in slow_functions:
            recommendations.append(OptimizationRecommendation(
                issue_type=PerformanceIssueType.INEFFICIENT_ALGORITHM,
                severity="critical" if metric.value > 1.0 else "high",
                description=f"í•¨ìˆ˜ '{metric.context['function']}'ì˜ ì‹¤í–‰ ì‹œê°„ì´ {metric.value:.2f}ì´ˆë¡œ ëŠë¦½ë‹ˆë‹¤",
                solution="ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë˜ëŠ” ìºì‹± ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”",
                estimated_improvement="50-70% ì„±ëŠ¥ í–¥ìƒ",
                implementation_code=self._generate_caching_code(metric.context['function'])
            ))
        
        return recommendations
    
    def _generate_refactoring_code(self, function_name: str) -> str:
        """ë¦¬íŒ©í† ë§ ì½”ë“œ ì˜ˆì‹œ ìƒì„±"""
        return f"""
# ë³µì¡í•œ í•¨ìˆ˜ë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
def {function_name}_optimized(data):
    # ë‹¨ê³„ 1: ë°ì´í„° ê²€ì¦
    validated_data = _validate_data(data)
    
    # ë‹¨ê³„ 2: í•µì‹¬ ë¡œì§ ì²˜ë¦¬
    result = _process_core_logic(validated_data)
    
    # ë‹¨ê³„ 3: ê²°ê³¼ í›„ì²˜ë¦¬
    return _post_process(result)

def _validate_data(data):
    # ê²€ì¦ ë¡œì§ ë¶„ë¦¬
    pass

def _process_core_logic(data):
    # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
    pass

def _post_process(result):
    # ê²°ê³¼ ì²˜ë¦¬
    pass
"""
    
    def _generate_caching_code(self, function_name: str) -> str:
        """ìºì‹± ì½”ë“œ ì˜ˆì‹œ ìƒì„±"""
        return f"""
from functools import lru_cache
import redis

# ë©”ëª¨ë¦¬ ìºì‹± (ê°„ë‹¨í•œ ê²½ìš°)
@lru_cache(maxsize=1000)
def {function_name}_cached(param):
    return {function_name}(param)

# Redis ìºì‹± (ë¶„ì‚° í™˜ê²½)
redis_client = redis.Redis(host='localhost', port=6379)

def {function_name}_redis_cached(param):
    cache_key = f"{function_name}:{{param}}"
    
    # ìºì‹œ í™•ì¸
    cached_result = redis_client.get(cache_key)
    if cached_result:
        return json.loads(cached_result)
    
    # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤í–‰ í›„ ì €ì¥
    result = {function_name}(param)
    redis_client.setex(cache_key, 3600, json.dumps(result))  # 1ì‹œê°„ TTL
    
    return result
"""

class DatabasePerformanceAnalyzer(PerformanceAnalyzer):
    """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
    
    async def analyze(self, queries: List[str]) -> List[PerformanceMetric]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë¶„ì„"""
        metrics = []
        
        for query in queries:
            # ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„
            explain_result = await self._explain_query(query)
            metrics.extend(explain_result)
            
            # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
            execution_time = await self._measure_query_time(query)
            metrics.append(PerformanceMetric(
                metric_name="query_execution_time",
                value=execution_time,
                unit="milliseconds",
                timestamp=datetime.now(),
                context={"query": query[:100]}  # ì¿¼ë¦¬ ì¼ë¶€ë§Œ ì €ì¥
            ))
        
        # ì¸ë±ìŠ¤ ì‚¬ìš© ë¶„ì„
        index_metrics = await self._analyze_index_usage()
        metrics.extend(index_metrics)
        
        return metrics
    
    async def _explain_query(self, query: str) -> List[PerformanceMetric]:
        """ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„"""
        import asyncpg
        
        metrics = []
        
        # PostgreSQL EXPLAIN ANALYZE
        try:
            conn = await asyncpg.connect(**self.db_config)
            explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}"
            result = await conn.fetchval(explain_query)
            await conn.close()
            
            plan = json.loads(result)[0]
            
            # ì‹¤í–‰ ê³„íšì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
            total_cost = plan['Plan']['Total Cost']
            execution_time = plan['Execution Time']
            
            metrics.append(PerformanceMetric(
                metric_name="query_cost",
                value=total_cost,
                unit="cost_units",
                timestamp=datetime.now(),
                context={"query": query[:100], "plan": plan}
            ))
            
            # í…Œì´ë¸” ìŠ¤ìº” í™•ì¸
            if self._has_sequential_scan(plan['Plan']):
                metrics.append(PerformanceMetric(
                    metric_name="sequential_scan_detected",
                    value=1,
                    unit="boolean",
                    timestamp=datetime.now(),
                    context={"query": query[:100], "issue": "Sequential scan detected"}
                ))
            
        except Exception as e:
            print(f"Query explain error: {e}")
        
        return metrics
    
    def _has_sequential_scan(self, plan: Dict) -> bool:
        """ìˆœì°¨ ìŠ¤ìº” ì—¬ë¶€ í™•ì¸"""
        if plan.get('Node Type') == 'Seq Scan':
            return True
        
        # í•˜ìœ„ í”Œëœ ì¬ê·€ì  í™•ì¸
        for child in plan.get('Plans', []):
            if self._has_sequential_scan(child):
                return True
        
        return False
    
    async def _analyze_index_usage(self) -> List[PerformanceMetric]:
        """ì¸ë±ìŠ¤ ì‚¬ìš© í˜„í™© ë¶„ì„"""
        import asyncpg
        
        metrics = []
        
        # PostgreSQL ì¸ë±ìŠ¤ í†µê³„
        query = """
        SELECT 
            schemaname,
            tablename,
            indexname,
            idx_scan,
            idx_tup_read,
            idx_tup_fetch
        FROM pg_stat_user_indexes
        WHERE idx_scan = 0
        ORDER BY schemaname, tablename;
        """
        
        try:
            conn = await asyncpg.connect(**self.db_config)
            unused_indexes = await conn.fetch(query)
            await conn.close()
            
            for idx in unused_indexes:
                metrics.append(PerformanceMetric(
                    metric_name="unused_index",
                    value=0,
                    unit="scans",
                    timestamp=datetime.now(),
                    context={
                        "index": idx['indexname'],
                        "table": idx['tablename'],
                        "schema": idx['schemaname']
                    }
                ))
        except Exception as e:
            print(f"Index analysis error: {e}")
        
        return metrics
    
    async def get_recommendations(self, metrics: List[PerformanceMetric]) -> List[OptimizationRecommendation]:
        """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # ëŠë¦° ì¿¼ë¦¬ ìµœì í™”
        slow_queries = [m for m in metrics if m.metric_name == "query_execution_time" and m.value > 100]
        for metric in slow_queries:
            recommendations.append(OptimizationRecommendation(
                issue_type=PerformanceIssueType.SLOW_QUERY,
                severity="critical" if metric.value > 1000 else "high",
                description=f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ {metric.value}msë¡œ ëŠë¦½ë‹ˆë‹¤",
                solution="ì¿¼ë¦¬ ìµœì í™” ë° ì¸ë±ìŠ¤ ì¶”ê°€ë¥¼ ê³ ë ¤í•˜ì„¸ìš”",
                estimated_improvement="70-90% ì„±ëŠ¥ í–¥ìƒ",
                implementation_code=self._generate_index_suggestion(metric.context.get('query', ''))
            ))
        
        # ìˆœì°¨ ìŠ¤ìº” ë¬¸ì œ
        seq_scans = [m for m in metrics if m.metric_name == "sequential_scan_detected"]
        for metric in seq_scans:
            recommendations.append(OptimizationRecommendation(
                issue_type=PerformanceIssueType.MISSING_INDEX,
                severity="high",
                description="í…Œì´ë¸” ì „ì²´ ìŠ¤ìº”ì´ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤",
                solution="ì ì ˆí•œ ì¸ë±ìŠ¤ë¥¼ ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•˜ì„¸ìš”",
                estimated_improvement="80-95% ì„±ëŠ¥ í–¥ìƒ",
                implementation_code=self._generate_index_creation(metric.context)
            ))
        
        # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤
        unused_indexes = [m for m in metrics if m.metric_name == "unused_index"]
        for metric in unused_indexes:
            recommendations.append(OptimizationRecommendation(
                issue_type=PerformanceIssueType.INEFFICIENT_ALGORITHM,
                severity="low",
                description=f"ì¸ë±ìŠ¤ '{metric.context['index']}'ê°€ ì‚¬ìš©ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤",
                solution="ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ë¥¼ ì œê±°í•˜ì—¬ ì“°ê¸° ì„±ëŠ¥ì„ ê°œì„ í•˜ì„¸ìš”",
                estimated_improvement="5-10% ì“°ê¸° ì„±ëŠ¥ í–¥ìƒ",
                implementation_code=f"DROP INDEX {metric.context['schema']}.{metric.context['index']};"
            ))
        
        return recommendations
    
    def _generate_index_suggestion(self, query: str) -> str:
        """ì¸ë±ìŠ¤ ìƒì„± ì œì•ˆ"""
        # ê°„ë‹¨í•œ WHERE ì ˆ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ íŒŒì„œ í•„ìš”)
        import re
        
        where_match = re.search(r'WHERE\s+(\w+)\s*=', query, re.IGNORECASE)
        if where_match:
            column = where_match.group(1)
            return f"""
-- WHERE ì ˆì— ì‚¬ìš©ëœ ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_{column} ON table_name({column});

-- ë³µí•© ì¸ë±ìŠ¤ê°€ í•„ìš”í•œ ê²½ìš°
CREATE INDEX idx_composite ON table_name(column1, column2);

-- ë¶€ë¶„ ì¸ë±ìŠ¤ (íŠ¹ì • ì¡°ê±´ì—ë§Œ ì ìš©)
CREATE INDEX idx_partial ON table_name(column) WHERE status = 'active';
"""
        
        return "-- ì¿¼ë¦¬ ë¶„ì„ í›„ ì ì ˆí•œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”"

class InfrastructurePerformanceAnalyzer(PerformanceAnalyzer):
    """ì¸í”„ë¼ ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    async def analyze(self, target_servers: List[str]) -> List[PerformanceMetric]:
        """ì¸í”„ë¼ ì„±ëŠ¥ ë¶„ì„"""
        metrics = []
        
        for server in target_servers:
            # CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            resource_metrics = await self._collect_resource_metrics(server)
            metrics.extend(resource_metrics)
            
            # ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì‹œê°„
            network_metrics = await self._measure_network_latency(server)
            metrics.extend(network_metrics)
            
            # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‘ë‹µ ì‹œê°„
            app_metrics = await self._measure_application_response(server)
            metrics.extend(app_metrics)
        
        return metrics
    
    async def _collect_resource_metrics(self, server: str) -> List[PerformanceMetric]:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ìˆ˜ì§‘"""
        metrics = []
        
        # ë¡œì»¬ ì„œë²„ì¸ ê²½ìš° psutil ì‚¬ìš©
        if server == "localhost":
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_info = psutil.virtual_memory()
            disk_info = psutil.disk_usage('/')
            
            metrics.extend([
                PerformanceMetric(
                    metric_name="cpu_usage",
                    value=cpu_percent,
                    unit="percent",
                    timestamp=datetime.now(),
                    context={"server": server}
                ),
                PerformanceMetric(
                    metric_name="memory_usage",
                    value=memory_info.percent,
                    unit="percent",
                    timestamp=datetime.now(),
                    context={"server": server, "used_gb": memory_info.used / (1024**3)}
                ),
                PerformanceMetric(
                    metric_name="disk_usage",
                    value=disk_info.percent,
                    unit="percent",
                    timestamp=datetime.now(),
                    context={"server": server, "free_gb": disk_info.free / (1024**3)}
                )
            ])
        
        return metrics
    
    async def _measure_network_latency(self, server: str) -> List[PerformanceMetric]:
        """ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì‹œê°„ ì¸¡ì •"""
        import aiohttp
        import time
        
        metrics = []
        
        # HTTP ì—”ë“œí¬ì¸íŠ¸ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        try:
            start_time = time.time()
            async with aiohttp.ClientSession() as session:
                async with session.get(f"http://{server}/health", timeout=10) as response:
                    latency = (time.time() - start_time) * 1000  # ms
                    
                    metrics.append(PerformanceMetric(
                        metric_name="network_latency",
                        value=latency,
                        unit="milliseconds",
                        timestamp=datetime.now(),
                        context={"server": server, "status_code": response.status}
                    ))
        except Exception as e:
            print(f"Network latency measurement error: {e}")
        
        return metrics
    
    async def get_recommendations(self, metrics: List[PerformanceMetric]) -> List[OptimizationRecommendation]:
        """ì¸í”„ë¼ ìµœì í™” ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # CPU ì‚¬ìš©ë¥  ë†’ìŒ
        high_cpu = [m for m in metrics if m.metric_name == "cpu_usage" and m.value > 80]
        for metric in high_cpu:
            recommendations.append(OptimizationRecommendation(
                issue_type=PerformanceIssueType.HIGH_CPU,
                severity="critical" if metric.value > 90 else "high",
                description=f"ì„œë²„ '{metric.context['server']}'ì˜ CPU ì‚¬ìš©ë¥ ì´ {metric.value}%ë¡œ ë†’ìŠµë‹ˆë‹¤",
                solution="ìŠ¤ì¼€ì¼ ì•„ì›ƒ ë˜ëŠ” CPU ì§‘ì•½ì  ì‘ì—… ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤",
                estimated_improvement="30-50% ì‘ë‹µ ì‹œê°„ ê°œì„ ",
                implementation_code=self._generate_scaling_config()
            ))
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ
        high_memory = [m for m in metrics if m.metric_name == "memory_usage" and m.value > 85]
        for metric in high_memory:
            recommendations.append(OptimizationRecommendation(
                issue_type=PerformanceIssueType.MEMORY_LEAK,
                severity="critical" if metric.value > 95 else "high",
                description=f"ì„œë²„ '{metric.context['server']}'ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ {metric.value}%ë¡œ ë†’ìŠµë‹ˆë‹¤",
                solution="ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸ ë° ë©”ëª¨ë¦¬ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤",
                estimated_improvement="ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 20-40% ê°ì†Œ",
                implementation_code=self._generate_memory_optimization()
            ))
        
        return recommendations
    
    def _generate_scaling_config(self) -> str:
        """ìŠ¤ì¼€ì¼ë§ ì„¤ì • ì˜ˆì‹œ"""
        return """
# Kubernetes HPA (Horizontal Pod Autoscaler)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
"""
    
    def _generate_memory_optimization(self) -> str:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì½”ë“œ"""
        return """
# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ë° ìµœì í™”

# 1. Python ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
from memory_profiler import profile

@profile
def memory_intensive_function():
    # í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
    pass

# 2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
import gc
import weakref

class ResourceManager:
    def __init__(self):
        self._resources = weakref.WeakValueDictionary()
    
    def get_resource(self, key):
        # ì•½í•œ ì°¸ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        return self._resources.get(key)
    
    def cleanup(self):
        # ëª…ì‹œì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()

# 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° êµ¬ì¡°
# ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” generator ì‚¬ìš©
def process_large_file(filename):
    with open(filename) as f:
        for line in f:  # ì „ì²´ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•ŠìŒ
            yield process_line(line)

# 4. ìºì‹œ í¬ê¸° ì œí•œ
from functools import lru_cache

@lru_cache(maxsize=1000)  # ìµœëŒ€ 1000ê°œ í•­ëª©ë§Œ ìºì‹œ
def expensive_operation(param):
    pass
"""

class PerformanceOptimizationAgent:
    """ì„±ëŠ¥ ìµœì í™” ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.code_analyzer = CodePerformanceAnalyzer()
        self.db_analyzer = None  # í•„ìš”ì‹œ ì´ˆê¸°í™”
        self.infra_analyzer = InfrastructurePerformanceAnalyzer()
        
    async def optimize_application(self, app_config: Dict[str, Any]) -> Dict[str, Any]:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ì²´ ì„±ëŠ¥ ìµœì í™”"""
        
        print("ğŸš€ ì„±ëŠ¥ ìµœì í™” ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        all_metrics = []
        all_recommendations = []
        
        # 1. ì½”ë“œ ì„±ëŠ¥ ë¶„ì„
        if 'code_paths' in app_config:
            print("ğŸ“Š ì½”ë“œ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
            for code_path in app_config['code_paths']:
                metrics = await self.code_analyzer.analyze(code_path)
                all_metrics.extend(metrics)
                
                recommendations = await self.code_analyzer.get_recommendations(metrics)
                all_recommendations.extend(recommendations)
        
        # 2. ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„
        if 'database' in app_config:
            print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
            self.db_analyzer = DatabasePerformanceAnalyzer(app_config['database'])
            
            if 'queries' in app_config:
                metrics = await self.db_analyzer.analyze(app_config['queries'])
                all_metrics.extend(metrics)
                
                recommendations = await self.db_analyzer.get_recommendations(metrics)
                all_recommendations.extend(recommendations)
        
        # 3. ì¸í”„ë¼ ì„±ëŠ¥ ë¶„ì„
        if 'servers' in app_config:
            print("ğŸ–¥ï¸ ì¸í”„ë¼ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
            metrics = await self.infra_analyzer.analyze(app_config['servers'])
            all_metrics.extend(metrics)
            
            recommendations = await self.infra_analyzer.get_recommendations(metrics)
            all_recommendations.extend(recommendations)
        
        # 4. ì¢…í•© ë¶„ì„ ë° ìš°ì„ ìˆœìœ„ ê²°ì •
        prioritized_recommendations = self._prioritize_recommendations(all_recommendations)
        
        # 5. ìµœì í™” ê³„íš ìƒì„±
        optimization_plan = self._create_optimization_plan(prioritized_recommendations)
        
        # 6. ìë™ ìµœì í™” ì‹¤í–‰ (ì„ íƒì )
        if app_config.get('auto_optimize', False):
            applied_optimizations = await self._apply_optimizations(optimization_plan)
        else:
            applied_optimizations = []
        
        # 7. ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        report = self._generate_performance_report(
            all_metrics,
            prioritized_recommendations,
            optimization_plan,
            applied_optimizations
        )
        
        print("âœ… ì„±ëŠ¥ ìµœì í™” ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        return report
    
    def _prioritize_recommendations(self, recommendations: List[OptimizationRecommendation]) -> List[OptimizationRecommendation]:
        """ê¶Œì¥ì‚¬í•­ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        
        # ì‹¬ê°ë„ì™€ ì˜ˆìƒ ê°œì„  íš¨ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        severity_score = {
            'critical': 4,
            'high': 3,
            'medium': 2,
            'low': 1
        }
        
        def score_recommendation(rec: OptimizationRecommendation) -> float:
            base_score = severity_score.get(rec.severity, 0)
            
            # ì˜ˆìƒ ê°œì„  íš¨ê³¼ì—ì„œ ìˆ«ì ì¶”ì¶œ
            import re
            improvement_match = re.search(r'(\d+)-(\d+)%', rec.estimated_improvement)
            if improvement_match:
                avg_improvement = (int(improvement_match.group(1)) + int(improvement_match.group(2))) / 2
                base_score += avg_improvement / 100
            
            # ìœ„í—˜ë„ ì¡°ì •
            if rec.risk_level == 'high':
                base_score *= 0.7
            elif rec.risk_level == 'medium':
                base_score *= 0.85
            
            return base_score
        
        return sorted(recommendations, key=score_recommendation, reverse=True)
    
    def _create_optimization_plan(self, recommendations: List[OptimizationRecommendation]) -> Dict[str, Any]:
        """ìµœì í™” ì‹¤í–‰ ê³„íš ìƒì„±"""
        
        plan = {
            'phases': [],
            'total_estimated_time': 0,
            'expected_improvements': {}
        }
        
        # Phase 1: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìµœì í™” (ë‚®ì€ ìœ„í—˜ë„)
        phase1 = {
            'name': 'Quick Wins',
            'duration': '1-2 hours',
            'tasks': []
        }
        
        for rec in recommendations:
            if rec.risk_level == 'low' and rec.severity in ['critical', 'high']:
                phase1['tasks'].append({
                    'type': rec.issue_type.value,
                    'description': rec.description,
                    'solution': rec.solution,
                    'code': rec.implementation_code
                })
        
        if phase1['tasks']:
            plan['phases'].append(phase1)
        
        # Phase 2: ì¤‘ê°„ ìœ„í—˜ë„ ìµœì í™”
        phase2 = {
            'name': 'Medium Risk Optimizations',
            'duration': '4-8 hours',
            'tasks': []
        }
        
        for rec in recommendations:
            if rec.risk_level == 'medium':
                phase2['tasks'].append({
                    'type': rec.issue_type.value,
                    'description': rec.description,
                    'solution': rec.solution,
                    'code': rec.implementation_code,
                    'testing_required': True
                })
        
        if phase2['tasks']:
            plan['phases'].append(phase2)
        
        # Phase 3: ì£¼ìš” ì•„í‚¤í…ì²˜ ë³€ê²½
        phase3 = {
            'name': 'Architecture Changes',
            'duration': '1-2 weeks',
            'tasks': []
        }
        
        for rec in recommendations:
            if rec.risk_level == 'high' or rec.issue_type in [
                PerformanceIssueType.INEFFICIENT_ALGORITHM,
                PerformanceIssueType.HIGH_CPU
            ]:
                phase3['tasks'].append({
                    'type': rec.issue_type.value,
                    'description': rec.description,
                    'solution': rec.solution,
                    'code': rec.implementation_code,
                    'approval_required': True
                })
        
        if phase3['tasks']:
            plan['phases'].append(phase3)
        
        return plan
    
    async def _apply_optimizations(self, plan: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìë™ìœ¼ë¡œ ì ìš© ê°€ëŠ¥í•œ ìµœì í™” ì‹¤í–‰"""
        
        applied = []
        
        # Phase 1ì˜ ë‚®ì€ ìœ„í—˜ë„ ìµœì í™”ë§Œ ìë™ ì ìš©
        if plan['phases'] and plan['phases'][0]['name'] == 'Quick Wins':
            for task in plan['phases'][0]['tasks']:
                if task['type'] in ['cache_miss', 'missing_index']:
                    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—¬ê¸°ì„œ ìµœì í™” ì½”ë“œ ì‹¤í–‰
                    applied.append({
                        'task': task['description'],
                        'status': 'applied',
                        'timestamp': datetime.now()
                    })
        
        return applied
    
    def _generate_performance_report(
        self,
        metrics: List[PerformanceMetric],
        recommendations: List[OptimizationRecommendation],
        plan: Dict[str, Any],
        applied_optimizations: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # ë©”íŠ¸ë¦­ ìš”ì•½
        metric_summary = {}
        for metric in metrics:
            if metric.metric_name not in metric_summary:
                metric_summary[metric.metric_name] = {
                    'values': [],
                    'avg': 0,
                    'max': 0,
                    'min': float('inf')
                }
            
            metric_summary[metric.metric_name]['values'].append(metric.value)
            metric_summary[metric.metric_name]['max'] = max(
                metric_summary[metric.metric_name]['max'],
                metric.value
            )
            metric_summary[metric.metric_name]['min'] = min(
                metric_summary[metric.metric_name]['min'],
                metric.value
            )
        
        # í‰ê·  ê³„ì‚°
        for metric_name, data in metric_summary.items():
            if data['values']:
                data['avg'] = sum(data['values']) / len(data['values'])
        
        # ì´ìŠˆ ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
        issue_summary = {}
        for rec in recommendations:
            issue_type = rec.issue_type.value
            if issue_type not in issue_summary:
                issue_summary[issue_type] = {
                    'count': 0,
                    'severities': {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
                }
            
            issue_summary[issue_type]['count'] += 1
            issue_summary[issue_type]['severities'][rec.severity] += 1
        
        report = {
            'summary': {
                'total_issues': len(recommendations),
                'critical_issues': sum(1 for r in recommendations if r.severity == 'critical'),
                'high_issues': sum(1 for r in recommendations if r.severity == 'high'),
                'applied_optimizations': len(applied_optimizations)
            },
            'metrics': metric_summary,
            'issues_by_type': issue_summary,
            'top_recommendations': recommendations[:5],
            'optimization_plan': plan,
            'applied_optimizations': applied_optimizations,
            'generated_at': datetime.now().isoformat()
        }
        
        # ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        with open('performance_report.json', 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_markdown_report(report)
        
        return report
    
    def _generate_markdown_report(self, report: Dict[str, Any]):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        markdown = f"""# ì„±ëŠ¥ ìµœì í™” ë¦¬í¬íŠ¸

ìƒì„±ì¼ì‹œ: {report['generated_at']}

## ğŸ“Š ìš”ì•½

- **ì´ ë°œê²¬ëœ ì´ìŠˆ**: {report['summary']['total_issues']}ê°œ
- **ì‹¬ê°**: {report['summary']['critical_issues']}ê°œ
- **ë†’ìŒ**: {report['summary']['high_issues']}ê°œ
- **ìë™ ì ìš©ëœ ìµœì í™”**: {report['summary']['applied_optimizations']}ê°œ

## ğŸ” ì£¼ìš” ë°œê²¬ ì‚¬í•­

### ìƒìœ„ 5ê°œ ê¶Œì¥ì‚¬í•­

"""
        
        for i, rec in enumerate(report['top_recommendations'], 1):
            markdown += f"""
{i}. **{rec['description']}**
   - ì‹¬ê°ë„: {rec['severity']}
   - ì˜ˆìƒ ê°œì„ : {rec['estimated_improvement']}
   - í•´ê²°ì±…: {rec['solution']}
"""
        
        markdown += """
## ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | í‰ê·  | ìµœëŒ€ | ìµœì†Œ |
|--------|------|------|------|
"""
        
        for metric_name, data in report['metrics'].items():
            markdown += f"| {metric_name} | {data['avg']:.2f} | {data['max']:.2f} | {data['min']:.2f} |\n"
        
        markdown += """
## ğŸ› ï¸ ìµœì í™” ê³„íš

"""
        
        for phase in report['optimization_plan']['phases']:
            markdown += f"### {phase['name']} (ì˜ˆìƒ ì†Œìš” ì‹œê°„: {phase['duration']})\n\n"
            for task in phase['tasks']:
                markdown += f"- {task['description']}\n"
        
        with open('performance_report.md', 'w') as f:
            f.write(markdown)

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    agent = PerformanceOptimizationAgent('configs/agents/performance-optimization-agent.yaml')
    
    app_config = {
        'code_paths': ['src/api/handlers.py', 'src/services/data_processor.py'],
        'database': {
            'host': 'localhost',
            'port': 5432,
            'database': 'myapp',
            'user': 'postgres',
            'password': 'password'
        },
        'queries': [
            "SELECT * FROM users WHERE created_at > '2024-01-01'",
            "SELECT u.*, o.* FROM users u JOIN orders o ON u.id = o.user_id"
        ],
        'servers': ['localhost'],
        'auto_optimize': False
    }
    
    report = await agent.optimize_application(app_config)
    print(f"\nâœ… ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: performance_report.md")

if __name__ == "__main__":
    asyncio.run(main())