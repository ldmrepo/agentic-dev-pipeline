# ì—ì´ì „í‹± ê°œë°œ íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ êµ¬í˜„ ê³„íš

## ğŸ“‹ ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ê°œìš”](#í”„ë¡œì íŠ¸-ê°œìš”)
2. [êµ¬í˜„ ë¡œë“œë§µ](#êµ¬í˜„-ë¡œë“œë§µ)
3. [Phase 1: ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶•](#phase-1-ê¸°ë°˜-ì¸í”„ë¼-êµ¬ì¶•)
4. [Phase 2: í•µì‹¬ ì—”ì§„ ê°œë°œ](#phase-2-í•µì‹¬-ì—”ì§„-ê°œë°œ)
5. [Phase 3: ì—ì´ì „íŠ¸ êµ¬í˜„](#phase-3-ì—ì´ì „íŠ¸-êµ¬í˜„)
6. [Phase 4: í†µí•© ë° ìµœì í™”](#phase-4-í†µí•©-ë°-ìµœì í™”)
7. [Phase 5: í”„ë¡œë•ì…˜ ì¤€ë¹„](#phase-5-í”„ë¡œë•ì…˜-ì¤€ë¹„)
8. [ë¦¬ìŠ¤í¬ ê´€ë¦¬](#ë¦¬ìŠ¤í¬-ê´€ë¦¬)
9. [ì„±ê³µ ì§€í‘œ](#ì„±ê³µ-ì§€í‘œ)

## í”„ë¡œì íŠ¸ ê°œìš”

### ëª©í‘œ
AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ìƒëª…ì£¼ê¸° ì „ì²´ë¥¼ ìë™í™”í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬ì¶•

### í•µì‹¬ ê°€ì¹˜
- **ê°œë°œ ì‹œê°„ ë‹¨ì¶•**: 2-4ì£¼ â†’ 2-4ì‹œê°„
- **í’ˆì§ˆ í–¥ìƒ**: í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 85%+ ìë™ ë‹¬ì„±
- **ë¹„ìš© ì ˆê°**: ê°œë°œ ë¹„ìš© 30-50% ê°ì†Œ

### ê¸°ìˆ  ìŠ¤íƒ
- **Orchestration**: LangGraph
- **Runtime**: Python 3.11+
- **API**: FastAPI
- **Storage**: PostgreSQL, Redis, ChromaDB
- **AI**: Claude API (Opus 4)
- **Integration**: MCP (Model Context Protocol)

## êµ¬í˜„ ë¡œë“œë§µ

```mermaid
gantt
    title ì—ì´ì „í‹± ê°œë°œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ì¼ì •
    dateFormat  YYYY-MM-DD
    section Phase 1
    ê¸°ë°˜ ì¸í”„ë¼           :p1, 2025-01-01, 2w
    section Phase 2
    í•µì‹¬ ì—”ì§„            :p2, after p1, 3w
    section Phase 3
    ì—ì´ì „íŠ¸ êµ¬í˜„         :p3, after p2, 3w
    section Phase 4
    í†µí•© ë° ìµœì í™”        :p4, after p3, 2w
    section Phase 5
    í”„ë¡œë•ì…˜ ì¤€ë¹„         :p5, after p4, 2w
```

## Phase 1: ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶•

### 1.1 í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì • (3ì¼)

#### ë””ë ‰í† ë¦¬ êµ¬ì¡°
```bash
agentic-dev-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                    # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py            # ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ dependencies.py    # ì˜ì¡´ì„± ì£¼ì…
â”‚   â”‚   â”œâ”€â”€ routes/            # API ë¼ìš°íŠ¸
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â”‚   â”œâ”€â”€ agents.py
â”‚   â”‚   â”‚   â””â”€â”€ monitoring.py
â”‚   â”‚   â””â”€â”€ middleware/        # ë¯¸ë“¤ì›¨ì–´
â”‚   â”‚       â”œâ”€â”€ auth.py
â”‚   â”‚       â”œâ”€â”€ logging.py
â”‚   â”‚       â””â”€â”€ error.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                  # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ config.py         # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ constants.py      # ìƒìˆ˜ ì •ì˜
â”‚   â”‚   â”œâ”€â”€ exceptions.py     # ì»¤ìŠ¤í…€ ì˜ˆì™¸
â”‚   â”‚   â””â”€â”€ schemas.py        # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestration/         # LangGraph ì›Œí¬í”Œë¡œìš°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ engine.py         # ì›Œí¬í”Œë¡œìš° ì—”ì§„
â”‚   â”‚   â”œâ”€â”€ graphs/           # ê·¸ë˜í”„ ì •ì˜
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hotfix.py
â”‚   â”‚   â”‚   â””â”€â”€ feature.py
â”‚   â”‚   â”œâ”€â”€ nodes/            # ë…¸ë“œ êµ¬í˜„
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”œâ”€â”€ planning.py
â”‚   â”‚   â”‚   â””â”€â”€ development.py
â”‚   â”‚   â””â”€â”€ state.py          # ìƒíƒœ ê´€ë¦¬
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/               # AI ì—ì´ì „íŠ¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py          # ë² ì´ìŠ¤ ì—ì´ì „íŠ¸
â”‚   â”‚   â”œâ”€â”€ planning/        # Planning Agent
â”‚   â”‚   â”œâ”€â”€ development/     # Development Agent
â”‚   â”‚   â”œâ”€â”€ testing/         # Testing Agent
â”‚   â”‚   â”œâ”€â”€ deployment/      # Deployment Agent
â”‚   â”‚   â””â”€â”€ monitoring/      # Monitoring Agent
â”‚   â”‚
â”‚   â”œâ”€â”€ integrations/        # ì™¸ë¶€ ì„œë¹„ìŠ¤ í†µí•©
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ claude.py       # Claude API í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ mcp/           # MCP í†µí•©
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ servers.py
â”‚   â”‚   â”‚   â””â”€â”€ tools.py
â”‚   â”‚   â””â”€â”€ tools/         # ë„êµ¬ ì–´ëŒ‘í„°
â”‚   â”‚       â”œâ”€â”€ github.py
â”‚   â”‚       â”œâ”€â”€ docker.py
â”‚   â”‚       â””â”€â”€ kubernetes.py
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/            # ë°ì´í„° ì €ì¥ì†Œ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py    # PostgreSQL
â”‚   â”‚   â”œâ”€â”€ cache.py       # Redis
â”‚   â”‚   â”œâ”€â”€ vector.py      # ChromaDB
â”‚   â”‚   â””â”€â”€ models.py      # SQLAlchemy ëª¨ë¸
â”‚   â”‚
â”‚   â””â”€â”€ utils/             # ìœ í‹¸ë¦¬í‹°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py      # ë¡œê¹… ì„¤ì •
â”‚       â”œâ”€â”€ metrics.py     # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
â”‚       â””â”€â”€ helpers.py     # í—¬í¼ í•¨ìˆ˜
â”‚
â”œâ”€â”€ tests/                 # í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”‚
â”œâ”€â”€ scripts/              # ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup.sh
â”‚   â”œâ”€â”€ migrate.py
â”‚   â””â”€â”€ seed.py
â”‚
â”œâ”€â”€ docker/              # Docker ì„¤ì •
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ k8s/                # Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ configmap.yaml
â”‚
â”œâ”€â”€ docs/               # ë¬¸ì„œ
â”œâ”€â”€ .env.example       # í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
â”œâ”€â”€ requirements.txt   # Python ì˜ì¡´ì„±
â”œâ”€â”€ pyproject.toml    # í”„ë¡œì íŠ¸ ì„¤ì •
â””â”€â”€ README.md         # í”„ë¡œì íŠ¸ ì„¤ëª…
```

#### ì´ˆê¸° ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
```python
# scripts/init_project.py
import os
import subprocess
from pathlib import Path

def create_project_structure():
    """í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
    base_dirs = [
        "src/api/routes",
        "src/api/middleware",
        "src/core",
        "src/orchestration/graphs",
        "src/orchestration/nodes",
        "src/agents/planning",
        "src/agents/development",
        "src/agents/testing",
        "src/agents/deployment",
        "src/agents/monitoring",
        "src/integrations/mcp",
        "src/integrations/tools",
        "src/storage",
        "src/utils",
        "tests/unit",
        "tests/integration",
        "tests/e2e",
        "scripts",
        "docker",
        "k8s",
        "docs"
    ]
    
    for dir_path in base_dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        # __init__.py ìƒì„±
        if dir_path.startswith("src/"):
            init_file = Path(dir_path) / "__init__.py"
            init_file.touch()

def setup_virtual_environment():
    """ê°€ìƒ í™˜ê²½ ì„¤ì •"""
    subprocess.run(["python", "-m", "venv", "venv"])
    print("Virtual environment created. Activate with: source venv/bin/activate")

def create_requirements_file():
    """requirements.txt ìƒì„±"""
    requirements = """
# Core
python-dotenv==1.0.0
pydantic==2.5.0
pydantic-settings==2.1.0

# Web Framework
fastapi==0.109.0
uvicorn[standard]==0.27.0
httpx==0.26.0

# Database
sqlalchemy==2.0.25
alembic==1.13.1
asyncpg==0.29.0
psycopg2-binary==2.9.9

# Cache
redis==5.0.1
hiredis==2.3.2

# Vector Store
chromadb==0.4.22

# AI/ML
langchain==0.1.0
langchain-anthropic==0.1.1
langgraph==0.0.26
openai==1.9.0

# Monitoring
prometheus-client==0.19.0
sentry-sdk==1.39.2

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0
pytest-mock==3.12.0

# Development
black==23.12.1
ruff==0.1.11
mypy==1.8.0
pre-commit==3.6.0
"""
    
    with open("requirements.txt", "w") as f:
        f.write(requirements.strip())

if __name__ == "__main__":
    create_project_structure()
    setup_virtual_environment()
    create_requirements_file()
    print("Project structure initialized successfully!")
```

### 1.2 ê°œë°œ í™˜ê²½ ì„¤ì • (2ì¼)

#### Docker Compose ì„¤ì •
```yaml
# docker/docker-compose.yml
version: '3.9'

services:
  # PostgreSQL
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: agentic_pipeline
      POSTGRES_USER: agentic
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U agentic"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ChromaDB
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      IS_PERSISTENT: TRUE
      ANONYMIZED_TELEMETRY: FALSE

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

  # Grafana
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning

  # Application
  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8080:8080"
    environment:
      DATABASE_URL: postgresql://agentic:${POSTGRES_PASSWORD}@postgres:5432/agentic_pipeline
      REDIS_URL: redis://redis:6379
      CHROMA_URL: http://chromadb:8000
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      chromadb:
        condition: service_started
    volumes:
      - ../src:/app/src
      - ../.claude:/app/.claude

volumes:
  postgres_data:
  redis_data:
  chroma_data:
  prometheus_data:
  grafana_data:
```

#### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# src/core/config.py
from pydantic_settings import BaseSettings
from typing import Optional, Dict, Any
from functools import lru_cache

class Settings(BaseSettings):
    # Application
    app_name: str = "Agentic Development Pipeline"
    app_version: str = "1.0.0"
    debug: bool = False
    environment: str = "development"
    
    # API
    api_v1_str: str = "/api/v1"
    api_key: Optional[str] = None
    
    # Database
    postgres_server: str = "localhost"
    postgres_user: str = "agentic"
    postgres_password: str
    postgres_db: str = "agentic_pipeline"
    
    @property
    def database_url(self) -> str:
        return f"postgresql+asyncpg://{self.postgres_user}:{self.postgres_password}@{self.postgres_server}/{self.postgres_db}"
    
    # Redis
    redis_url: str = "redis://localhost:6379"
    redis_ttl: int = 3600  # 1 hour
    
    # ChromaDB
    chroma_host: str = "localhost"
    chroma_port: int = 8000
    
    # Claude API
    anthropic_api_key: str
    claude_model: str = "claude-3-opus-20240229"
    claude_max_tokens: int = 4096
    claude_temperature: float = 0.1
    
    # LangGraph
    langgraph_recursion_limit: int = 50
    langgraph_checkpoint_ttl: int = 86400  # 24 hours
    
    # MCP
    mcp_servers: Dict[str, Dict[str, Any]] = {
        "filesystem": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem"],
            "env": {}
        },
        "github": {
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {"GITHUB_PERSONAL_ACCESS_TOKEN": "${GITHUB_TOKEN}"}
        }
    }
    
    # Security
    secret_key: str
    access_token_expire_minutes: int = 30
    refresh_token_expire_days: int = 7
    
    # Monitoring
    sentry_dsn: Optional[str] = None
    prometheus_port: int = 8081
    
    # Rate Limiting
    rate_limit_requests: int = 100
    rate_limit_period: int = 60  # seconds
    
    class Config:
        env_file = ".env"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    """ìºì‹œëœ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return Settings()

# ì„¤ì • ê²€ì¦
def validate_settings():
    """ì„¤ì • ê°’ ê²€ì¦"""
    settings = get_settings()
    
    # í•„ìˆ˜ ê°’ í™•ì¸
    required = [
        "anthropic_api_key",
        "postgres_password",
        "secret_key"
    ]
    
    for field in required:
        if not getattr(settings, field):
            raise ValueError(f"Required setting '{field}' is not set")
    
    print("âœ… All settings validated successfully")
```

### 1.3 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„ (2ì¼)

#### SQLAlchemy ëª¨ë¸
```python
# src/storage/models.py
from sqlalchemy import Column, String, DateTime, JSON, Enum, ForeignKey, Integer, Boolean, Text
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime
import uuid
import enum

Base = declarative_base()

class PipelineStatus(enum.Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class AgentType(enum.Enum):
    PLANNING = "planning"
    DEVELOPMENT = "development"
    TESTING = "testing"
    DEPLOYMENT = "deployment"
    MONITORING = "monitoring"

class Pipeline(Base):
    __tablename__ = "pipelines"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False)
    description = Column(Text)
    status = Column(Enum(PipelineStatus), default=PipelineStatus.PENDING)
    config = Column(JSON, default={})
    input_data = Column(JSON, default={})
    output_data = Column(JSON, default={})
    error_message = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    executions = relationship("AgentExecution", back_populates="pipeline")
    checkpoints = relationship("Checkpoint", back_populates="pipeline")

class AgentExecution(Base):
    __tablename__ = "agent_executions"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    pipeline_id = Column(UUID(as_uuid=True), ForeignKey("pipelines.id"))
    agent_type = Column(Enum(AgentType), nullable=False)
    status = Column(Enum(PipelineStatus), default=PipelineStatus.PENDING)
    input_data = Column(JSON, default={})
    output_data = Column(JSON, default={})
    error_message = Column(Text)
    execution_time = Column(Integer)  # milliseconds
    token_usage = Column(JSON, default={})  # {"input": 0, "output": 0}
    created_at = Column(DateTime, default=datetime.utcnow)
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    
    # Relationships
    pipeline = relationship("Pipeline", back_populates="executions")
    artifacts = relationship("Artifact", back_populates="execution")

class Artifact(Base):
    __tablename__ = "artifacts"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    execution_id = Column(UUID(as_uuid=True), ForeignKey("agent_executions.id"))
    name = Column(String(255), nullable=False)
    type = Column(String(50))  # code, document, test, config
    content = Column(Text)
    metadata = Column(JSON, default={})
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    execution = relationship("AgentExecution", back_populates="artifacts")

class Checkpoint(Base):
    __tablename__ = "checkpoints"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    pipeline_id = Column(UUID(as_uuid=True), ForeignKey("pipelines.id"))
    thread_id = Column(String(255), nullable=False)
    checkpoint_data = Column(JSON, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    pipeline = relationship("Pipeline", back_populates="checkpoints")

class MCPServer(Base):
    __tablename__ = "mcp_servers"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(100), unique=True, nullable=False)
    command = Column(String(255), nullable=False)
    args = Column(JSON, default=[])
    env = Column(JSON, default={})
    is_active = Column(Boolean, default=True)
    health_check_url = Column(String(255))
    last_health_check = Column(DateTime)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

#### ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
```python
# scripts/create_migration.py
import subprocess
import sys
from datetime import datetime

def create_migration(message: str):
    """Alembic ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    revision_message = f"{timestamp}_{message.replace(' ', '_').lower()}"
    
    subprocess.run([
        "alembic", "revision",
        "--autogenerate",
        "-m", revision_message
    ])
    
    print(f"Migration created: {revision_message}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python create_migration.py 'migration message'")
        sys.exit(1)
    
    create_migration(sys.argv[1])
```

### 1.4 CI/CD íŒŒì´í”„ë¼ì¸ ì„¤ì • (2ì¼)

#### GitHub Actions ì›Œí¬í”Œë¡œìš°
```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.11'

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install ruff black mypy
    
    - name: Run Ruff
      run: ruff check src tests
    
    - name: Run Black
      run: black --check src tests
    
    - name: Run MyPy
      run: mypy src

  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:test@localhost/test
        REDIS_URL: redis://localhost:6379
      run: |
        pytest tests/unit -v --cov=src --cov-report=xml
        pytest tests/integration -v
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  build:
    needs: [lint, test]
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile
        push: false
        tags: agentic-pipeline:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
```

## Phase 2: í•µì‹¬ ì—”ì§„ ê°œë°œ

### 2.1 LangGraph ì›Œí¬í”Œë¡œìš° ì—”ì§„ (5ì¼)

#### ë² ì´ìŠ¤ ì›Œí¬í”Œë¡œìš° ì—”ì§„
```python
# src/orchestration/engine.py
from typing import Dict, Any, Optional, List
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.prebuilt import ToolExecutor
import asyncio
import logging

from src.core.config import get_settings
from src.orchestration.state import WorkflowState
from src.storage.database import get_async_session

logger = logging.getLogger(__name__)
settings = get_settings()

class WorkflowEngine:
    """LangGraph ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì—”ì§„"""
    
    def __init__(self):
        self.graphs: Dict[str, StateGraph] = {}
        self.checkpointer = None
        self._initialize_checkpointer()
    
    def _initialize_checkpointer(self):
        """ì²´í¬í¬ì¸í„° ì´ˆê¸°í™”"""
        self.checkpointer = PostgresSaver.from_conn_string(
            settings.database_url.replace("+asyncpg", "")
        )
    
    def register_graph(self, name: str, graph: StateGraph):
        """ê·¸ë˜í”„ ë“±ë¡"""
        compiled = graph.compile(checkpointer=self.checkpointer)
        self.graphs[name] = compiled
        logger.info(f"Graph '{name}' registered successfully")
    
    async def execute(
        self,
        graph_name: str,
        input_data: Dict[str, Any],
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        if graph_name not in self.graphs:
            raise ValueError(f"Unknown graph: {graph_name}")
        
        graph = self.graphs[graph_name]
        
        # ê¸°ë³¸ ì„¤ì •
        if config is None:
            config = {}
        
        # thread_id ìƒì„±
        if "configurable" not in config:
            config["configurable"] = {}
        if "thread_id" not in config["configurable"]:
            config["configurable"]["thread_id"] = f"{graph_name}-{asyncio.get_event_loop().time()}"
        
        try:
            # ë¹„ë™ê¸° ì‹¤í–‰
            result = await graph.ainvoke(input_data, config)
            return result
        except Exception as e:
            logger.error(f"Workflow execution failed: {str(e)}")
            raise
    
    async def stream(
        self,
        graph_name: str,
        input_data: Dict[str, Any],
        config: Optional[Dict[str, Any]] = None
    ):
        """ì›Œí¬í”Œë¡œìš° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰"""
        if graph_name not in self.graphs:
            raise ValueError(f"Unknown graph: {graph_name}")
        
        graph = self.graphs[graph_name]
        
        async for chunk in graph.astream(input_data, config):
            yield chunk
    
    async def get_state(
        self,
        graph_name: str,
        thread_id: str
    ) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ"""
        if graph_name not in self.graphs:
            raise ValueError(f"Unknown graph: {graph_name}")
        
        graph = self.graphs[graph_name]
        config = {"configurable": {"thread_id": thread_id}}
        
        state = await graph.aget_state(config)
        return state.values if state else None
    
    async def update_state(
        self,
        graph_name: str,
        thread_id: str,
        updates: Dict[str, Any]
    ):
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if graph_name not in self.graphs:
            raise ValueError(f"Unknown graph: {graph_name}")
        
        graph = self.graphs[graph_name]
        config = {"configurable": {"thread_id": thread_id}}
        
        await graph.aupdate_state(config, updates)

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
workflow_engine = WorkflowEngine()
```

#### ë©”ì¸ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„
```python
# src/orchestration/graphs/main.py
from langgraph.graph import StateGraph, END
from typing import Dict, Any, List
import logging

from src.orchestration.state import WorkflowState
from src.orchestration.nodes import (
    analyze_task_node,
    planning_node,
    development_node,
    testing_node,
    deployment_node,
    monitoring_node,
    review_node
)

logger = logging.getLogger(__name__)

def create_main_workflow() -> StateGraph:
    """ë©”ì¸ ê°œë°œ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    
    # ê·¸ë˜í”„ ì´ˆê¸°í™”
    graph = StateGraph(WorkflowState)
    
    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("analyze_task", analyze_task_node)
    graph.add_node("planning", planning_node)
    graph.add_node("development", development_node)
    graph.add_node("testing", testing_node)
    graph.add_node("deployment", deployment_node)
    graph.add_node("monitoring", monitoring_node)
    graph.add_node("review", review_node)
    
    # ì—£ì§€ ì •ì˜
    graph.set_entry_point("analyze_task")
    
    # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
    graph.add_conditional_edges(
        "analyze_task",
        route_after_analysis,
        {
            "planning": "planning",
            "hotfix": "development",  # í•«í”½ìŠ¤ëŠ” ë°”ë¡œ ê°œë°œë¡œ
            "review": "review"
        }
    )
    
    # ìˆœì°¨ ì‹¤í–‰
    graph.add_edge("planning", "development")
    graph.add_edge("development", "testing")
    graph.add_edge("testing", "review")
    
    # ë¦¬ë·° í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ…
    graph.add_conditional_edges(
        "review",
        route_after_review,
        {
            "deployment": "deployment",
            "rework": "development",
            "end": END
        }
    )
    
    graph.add_edge("deployment", "monitoring")
    graph.add_edge("monitoring", END)
    
    return graph

def route_after_analysis(state: WorkflowState) -> str:
    """íƒœìŠ¤í¬ ë¶„ì„ í›„ ë¼ìš°íŒ…"""
    task_type = state.get("task_type", "")
    
    if task_type == "hotfix":
        logger.info("Routing to hotfix flow")
        return "hotfix"
    elif task_type == "review_only":
        logger.info("Routing to review only")
        return "review"
    else:
        logger.info("Routing to standard planning flow")
        return "planning"

def route_after_review(state: WorkflowState) -> str:
    """ë¦¬ë·° í›„ ë¼ìš°íŒ…"""
    review_result = state.get("review_result", {})
    
    if review_result.get("approved", False):
        logger.info("Review approved, proceeding to deployment")
        return "deployment"
    elif review_result.get("needs_rework", False):
        logger.info("Review requires rework")
        return "rework"
    else:
        logger.info("Review completed, ending workflow")
        return "end"

# ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ ì„œë¸Œê·¸ë˜í”„
def create_parallel_development_graph() -> StateGraph:
    """ë³‘ë ¬ ê°œë°œ ì›Œí¬í”Œë¡œìš° (í”„ë¡ íŠ¸ì—”ë“œ/ë°±ì—”ë“œ/ì¸í”„ë¼)"""
    
    graph = StateGraph(WorkflowState)
    
    # ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ
    graph.add_node("split_tasks", split_development_tasks)
    graph.add_node("frontend", frontend_development_node)
    graph.add_node("backend", backend_development_node)
    graph.add_node("infrastructure", infrastructure_development_node)
    graph.add_node("merge_results", merge_development_results)
    
    # í”Œë¡œìš° ì •ì˜
    graph.set_entry_point("split_tasks")
    
    # ë³‘ë ¬ ì‹¤í–‰ (Send API ì‚¬ìš©)
    graph.add_edge("split_tasks", "frontend")
    graph.add_edge("split_tasks", "backend")
    graph.add_edge("split_tasks", "infrastructure")
    
    # ê²°ê³¼ ë³‘í•©
    graph.add_edge("frontend", "merge_results")
    graph.add_edge("backend", "merge_results")
    graph.add_edge("infrastructure", "merge_results")
    
    graph.add_edge("merge_results", END)
    
    return graph
```

### 2.2 ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ (3ì¼)

#### ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜
```python
# src/orchestration/state.py
from typing import TypedDict, List, Dict, Any, Optional
from datetime import datetime
from pydantic import BaseModel
from enum import Enum

class TaskType(str, Enum):
    FEATURE = "feature"
    BUGFIX = "bugfix"
    HOTFIX = "hotfix"
    REFACTOR = "refactor"
    DOCUMENTATION = "documentation"

class AgentMessage(BaseModel):
    """ì—ì´ì „íŠ¸ ë©”ì‹œì§€"""
    agent: str
    content: str
    timestamp: datetime
    metadata: Dict[str, Any] = {}

class WorkflowState(TypedDict):
    """ì›Œí¬í”Œë¡œìš° ì „ì²´ ìƒíƒœ"""
    # ê¸°ë³¸ ì •ë³´
    pipeline_id: str
    thread_id: str
    task_type: TaskType
    status: str
    
    # ì…ë ¥ ë°ì´í„°
    requirements: str
    context: Dict[str, Any]
    constraints: List[str]
    
    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬
    messages: List[AgentMessage]
    
    # ê° ë‹¨ê³„ë³„ ê²°ê³¼
    analysis_result: Dict[str, Any]
    planning_result: Dict[str, Any]
    development_result: Dict[str, Any]
    testing_result: Dict[str, Any]
    review_result: Dict[str, Any]
    deployment_result: Dict[str, Any]
    
    # ì•„í‹°íŒ©íŠ¸
    artifacts: List[Dict[str, Any]]
    
    # ë©”íƒ€ë°ì´í„°
    created_at: datetime
    updated_at: datetime
    execution_time: int
    token_usage: Dict[str, int]
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: List[Dict[str, Any]]
    retry_count: int

class StateManager:
    """ìƒíƒœ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def create_initial_state(
        pipeline_id: str,
        requirements: str,
        task_type: TaskType = TaskType.FEATURE
    ) -> WorkflowState:
        """ì´ˆê¸° ìƒíƒœ ìƒì„±"""
        return WorkflowState(
            pipeline_id=pipeline_id,
            thread_id=f"{pipeline_id}-{datetime.utcnow().timestamp()}",
            task_type=task_type,
            status="initialized",
            requirements=requirements,
            context={},
            constraints=[],
            messages=[],
            analysis_result={},
            planning_result={},
            development_result={},
            testing_result={},
            review_result={},
            deployment_result={},
            artifacts=[],
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
            execution_time=0,
            token_usage={"input": 0, "output": 0},
            errors=[],
            retry_count=0
        )
    
    @staticmethod
    def add_message(
        state: WorkflowState,
        agent: str,
        content: str,
        metadata: Dict[str, Any] = None
    ) -> WorkflowState:
        """ë©”ì‹œì§€ ì¶”ê°€"""
        message = AgentMessage(
            agent=agent,
            content=content,
            timestamp=datetime.utcnow(),
            metadata=metadata or {}
        )
        
        state["messages"].append(message)
        state["updated_at"] = datetime.utcnow()
        
        return state
    
    @staticmethod
    def add_artifact(
        state: WorkflowState,
        name: str,
        type: str,
        content: Any,
        metadata: Dict[str, Any] = None
    ) -> WorkflowState:
        """ì•„í‹°íŒ©íŠ¸ ì¶”ê°€"""
        artifact = {
            "name": name,
            "type": type,
            "content": content,
            "metadata": metadata or {},
            "created_at": datetime.utcnow()
        }
        
        state["artifacts"].append(artifact)
        state["updated_at"] = datetime.utcnow()
        
        return state
    
    @staticmethod
    def update_token_usage(
        state: WorkflowState,
        input_tokens: int,
        output_tokens: int
    ) -> WorkflowState:
        """í† í° ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸"""
        state["token_usage"]["input"] += input_tokens
        state["token_usage"]["output"] += output_tokens
        
        return state
```

### 2.3 ë…¸ë“œ êµ¬í˜„ (5ì¼)

#### ë² ì´ìŠ¤ ë…¸ë“œ
```python
# src/orchestration/nodes/base.py
from typing import Dict, Any, Optional
from abc import ABC, abstractmethod
import logging
import time
from functools import wraps

from src.orchestration.state import WorkflowState, StateManager
from src.core.exceptions import NodeExecutionError

logger = logging.getLogger(__name__)

def node_error_handler(func):
    """ë…¸ë“œ ì—ëŸ¬ ì²˜ë¦¬ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def wrapper(state: WorkflowState, *args, **kwargs) -> Dict[str, Any]:
        start_time = time.time()
        node_name = func.__name__
        
        try:
            logger.info(f"Starting node: {node_name}")
            result = await func(state, *args, **kwargs)
            
            execution_time = int((time.time() - start_time) * 1000)
            logger.info(f"Node {node_name} completed in {execution_time}ms")
            
            # ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
            state["execution_time"] += execution_time
            
            return result
            
        except Exception as e:
            logger.error(f"Node {node_name} failed: {str(e)}")
            
            # ì—ëŸ¬ ê¸°ë¡
            error = {
                "node": node_name,
                "error": str(e),
                "timestamp": time.time()
            }
            state["errors"].append(error)
            state["status"] = "error"
            
            # ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì¦ê°€
            state["retry_count"] += 1
            
            raise NodeExecutionError(f"Node {node_name} execution failed: {str(e)}")
    
    return wrapper

class BaseNode(ABC):
    """ëª¨ë“  ë…¸ë“œì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"{__name__}.{name}")
    
    @abstractmethod
    async def execute(self, state: WorkflowState) -> Dict[str, Any]:
        """ë…¸ë“œ ì‹¤í–‰ ë¡œì§"""
        pass
    
    async def __call__(self, state: WorkflowState) -> Dict[str, Any]:
        """ë…¸ë“œ í˜¸ì¶œ"""
        return await self.execute(state)
    
    def add_message(self, state: WorkflowState, content: str, metadata: Dict[str, Any] = None):
        """ìƒíƒœì— ë©”ì‹œì§€ ì¶”ê°€"""
        StateManager.add_message(state, self.name, content, metadata)
    
    def add_artifact(self, state: WorkflowState, name: str, type: str, content: Any, metadata: Dict[str, Any] = None):
        """ìƒíƒœì— ì•„í‹°íŒ©íŠ¸ ì¶”ê°€"""
        StateManager.add_artifact(state, name, type, content, metadata)
```

#### íƒœìŠ¤í¬ ë¶„ì„ ë…¸ë“œ
```python
# src/orchestration/nodes/analyze.py
from typing import Dict, Any
import json

from src.orchestration.nodes.base import BaseNode, node_error_handler
from src.orchestration.state import WorkflowState, TaskType
from src.integrations.claude import claude_client

class AnalyzeTaskNode(BaseNode):
    """íƒœìŠ¤í¬ ë¶„ì„ ë…¸ë“œ"""
    
    def __init__(self):
        super().__init__("AnalyzeTask")
        
    @node_error_handler
    async def execute(self, state: WorkflowState) -> Dict[str, Any]:
        """íƒœìŠ¤í¬ ë¶„ì„ ì‹¤í–‰"""
        requirements = state["requirements"]
        
        # Claude APIë¥¼ ì‚¬ìš©í•œ ìš”êµ¬ì‚¬í•­ ë¶„ì„
        prompt = f"""
        Analyze the following requirements and provide a structured analysis:
        
        Requirements: {requirements}
        
        Please provide:
        1. Task type classification (feature/bugfix/hotfix/refactor/documentation)
        2. Complexity assessment (simple/medium/complex)
        3. Key technical requirements
        4. Potential risks and challenges
        5. Estimated effort (in hours)
        6. Required expertise areas
        
        Format the response as JSON.
        """
        
        response = await claude_client.analyze(prompt)
        analysis = json.loads(response)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["task_type"] = TaskType(analysis["task_type"])
        state["analysis_result"] = analysis
        
        # ë©”ì‹œì§€ ì¶”ê°€
        self.add_message(
            state,
            f"Task analyzed: {analysis['task_type']} - {analysis['complexity']}",
            {"analysis": analysis}
        )
        
        return {
            "analysis_result": analysis,
            "task_type": analysis["task_type"],
            "status": "analyzed"
        }

# ë…¸ë“œ ì¸ìŠ¤í„´ìŠ¤
analyze_task_node = AnalyzeTaskNode()
```

## Phase 3: ì—ì´ì „íŠ¸ êµ¬í˜„

### 3.1 ë² ì´ìŠ¤ ì—ì´ì „íŠ¸ (3ì¼)

#### ì—ì´ì „íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤
```python
# src/agents/base.py
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
import logging
from tenacity import retry, stop_after_attempt, wait_exponential

from langchain_anthropic import ChatAnthropic
from langchain.tools import Tool
from langgraph.prebuilt import create_react_agent

from src.core.config import get_settings
from src.integrations.mcp import MCPClient

settings = get_settings()
logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """ëª¨ë“  AI ì—ì´ì „íŠ¸ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description
        self.logger = logging.getLogger(f"agents.{name}")
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatAnthropic(
            model=settings.claude_model,
            anthropic_api_key=settings.anthropic_api_key,
            max_tokens=settings.claude_max_tokens,
            temperature=settings.claude_temperature
        )
        
        # MCP í´ë¼ì´ì–¸íŠ¸
        self.mcp_client = MCPClient(settings.mcp_servers)
        
        # ë„êµ¬ ì´ˆê¸°í™”
        self.tools = self._initialize_tools()
        
        # ReAct ì—ì´ì „íŠ¸ ìƒì„±
        self.agent = self._create_agent()
    
    @abstractmethod
    def _get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜"""
        pass
    
    @abstractmethod
    def _get_tools(self) -> List[Tool]:
        """ì—ì´ì „íŠ¸ ì „ìš© ë„êµ¬ ì •ì˜"""
        pass
    
    def _initialize_tools(self) -> List[Tool]:
        """ë„êµ¬ ì´ˆê¸°í™”"""
        base_tools = [
            Tool(
                name="mcp_filesystem",
                description="Access filesystem through MCP",
                func=self._mcp_filesystem_tool
            ),
            Tool(
                name="mcp_github",
                description="Access GitHub through MCP",
                func=self._mcp_github_tool
            )
        ]
        
        # ì—ì´ì „íŠ¸ë³„ ë„êµ¬ ì¶”ê°€
        agent_tools = self._get_tools()
        
        return base_tools + agent_tools
    
    def _create_agent(self):
        """ReAct ì—ì´ì „íŠ¸ ìƒì„±"""
        return create_react_agent(
            self.llm,
            self.tools,
            messages_modifier=self._get_system_prompt()
        )
    
    async def _mcp_filesystem_tool(self, operation: str, path: str, content: Optional[str] = None) -> str:
        """MCP íŒŒì¼ì‹œìŠ¤í…œ ë„êµ¬"""
        return await self.mcp_client.call_tool(
            "filesystem",
            operation,
            {"path": path, "content": content}
        )
    
    async def _mcp_github_tool(self, operation: str, repo: str, **kwargs) -> str:
        """MCP GitHub ë„êµ¬"""
        return await self.mcp_client.call_tool(
            "github",
            operation,
            {"repo": repo, **kwargs}
        )
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def execute(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        self.logger.info(f"Executing task: {task.get('description', 'No description')}")
        
        try:
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = await self.agent.ainvoke(task)
            
            # ê²°ê³¼ ì²˜ë¦¬
            processed_result = await self._process_result(result)
            
            self.logger.info("Task execution completed successfully")
            return processed_result
            
        except Exception as e:
            self.logger.error(f"Task execution failed: {str(e)}")
            raise
    
    @abstractmethod
    async def _process_result(self, result: Any) -> Dict[str, Any]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        pass
    
    async def validate_output(self, output: Dict[str, Any]) -> bool:
        """ì¶œë ¥ ê²€ì¦"""
        # ê¸°ë³¸ ê²€ì¦ ë¡œì§
        if not output:
            return False
        
        if "error" in output:
            return False
        
        # ì—ì´ì „íŠ¸ë³„ ì¶”ê°€ ê²€ì¦ì€ ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„
        return await self._validate_specific_output(output)
    
    async def _validate_specific_output(self, output: Dict[str, Any]) -> bool:
        """ì—ì´ì „íŠ¸ë³„ ì¶œë ¥ ê²€ì¦ (ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)"""
        return True
```

### 3.2 Planning Agent (3ì¼)

```python
# src/agents/planning/agent.py
from typing import List, Dict, Any
import json

from src.agents.base import BaseAgent
from langchain.tools import Tool
from src.integrations.tools.architecture import ArchitectureTool
from src.integrations.tools.estimation import EstimationTool

class PlanningAgent(BaseAgent):
    """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="PlanningAgent",
            description="Analyzes requirements and creates development plans"
        )
        self.architecture_tool = ArchitectureTool()
        self.estimation_tool = EstimationTool()
    
    def _get_system_prompt(self) -> str:
        return """You are a Planning Agent responsible for analyzing requirements and creating comprehensive development plans.

Your responsibilities:
1. Analyze functional and non-functional requirements
2. Design system architecture
3. Create work breakdown structure (WBS)
4. Estimate timelines and resources
5. Identify risks and dependencies
6. Define success criteria

Always provide structured, actionable plans with clear milestones.
Consider scalability, maintainability, and security in your designs.
"""
    
    def _get_tools(self) -> List[Tool]:
        return [
            Tool(
                name="create_architecture",
                description="Create system architecture diagram",
                func=self.architecture_tool.create_diagram
            ),
            Tool(
                name="estimate_effort",
                description="Estimate development effort",
                func=self.estimation_tool.estimate
            ),
            Tool(
                name="identify_risks",
                description="Identify project risks",
                func=self._identify_risks
            ),
            Tool(
                name="create_wbs",
                description="Create work breakdown structure",
                func=self._create_wbs
            )
        ]
    
    async def _identify_risks(self, requirements: str) -> str:
        """ë¦¬ìŠ¤í¬ ì‹ë³„"""
        prompt = f"""
        Identify potential risks for the following requirements:
        {requirements}
        
        Categories to consider:
        - Technical risks
        - Resource risks
        - Timeline risks
        - Integration risks
        - Security risks
        """
        
        response = await self.llm.ainvoke(prompt)
        return response.content
    
    async def _create_wbs(self, requirements: str) -> str:
        """ì‘ì—… ë¶„í•´ êµ¬ì¡° ìƒì„±"""
        prompt = f"""
        Create a detailed Work Breakdown Structure (WBS) for:
        {requirements}
        
        Format as hierarchical JSON with:
        - Task ID
        - Task name
        - Description
        - Estimated hours
        - Dependencies
        - Assignee type (frontend/backend/devops)
        """
        
        response = await self.llm.ainvoke(prompt)
        return response.content
    
    async def _process_result(self, result: Any) -> Dict[str, Any]:
        """ê³„íš ê²°ê³¼ ì²˜ë¦¬"""
        return {
            "architecture": result.get("architecture", {}),
            "wbs": result.get("wbs", {}),
            "timeline": result.get("timeline", {}),
            "risks": result.get("risks", []),
            "resources": result.get("resources", {}),
            "success_criteria": result.get("success_criteria", [])
        }
    
    async def _validate_specific_output(self, output: Dict[str, Any]) -> bool:
        """ê³„íš ì¶œë ¥ ê²€ì¦"""
        required_fields = ["architecture", "wbs", "timeline", "risks"]
        
        for field in required_fields:
            if field not in output or not output[field]:
                self.logger.error(f"Missing required field: {field}")
                return False
        
        # WBS ê²€ì¦
        if "tasks" not in output["wbs"] or len(output["wbs"]["tasks"]) == 0:
            self.logger.error("WBS must contain at least one task")
            return False
        
        return True
```

### 3.3 Development Agent (3ì¼)

```python
# src/agents/development/agent.py
from typing import List, Dict, Any
import os
import json

from src.agents.base import BaseAgent
from langchain.tools import Tool
from src.integrations.tools.code_generator import CodeGenerator
from src.integrations.tools.dependency_manager import DependencyManager

class DevelopmentAgent(BaseAgent):
    """ì½”ë“œ ìƒì„± ë° êµ¬í˜„ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__(
            name="DevelopmentAgent",
            description="Generates code and implements features"
        )
        self.code_generator = CodeGenerator()
        self.dependency_manager = DependencyManager()
    
    def _get_system_prompt(self) -> str:
        return """You are a Development Agent responsible for implementing code based on specifications.

Your responsibilities:
1. Generate clean, maintainable code
2. Follow best practices and design patterns
3. Implement proper error handling
4. Add appropriate logging and monitoring
5. Write self-documenting code
6. Ensure code is testable

Technologies you work with:
- Backend: Python (FastAPI), Node.js (Express)
- Frontend: React, TypeScript
- Database: PostgreSQL, Redis
- Infrastructure: Docker, Kubernetes

Always follow the project's coding standards and architecture decisions.
"""
    
    def _get_tools(self) -> List[Tool]:
        return [
            Tool(
                name="generate_code",
                description="Generate code based on specifications",
                func=self.code_generator.generate
            ),
            Tool(
                name="create_api_endpoint",
                description="Create API endpoint with FastAPI",
                func=self._create_api_endpoint
            ),
            Tool(
                name="create_react_component",
                description="Create React component",
                func=self._create_react_component
            ),
            Tool(
                name="setup_database_schema",
                description="Setup database schema",
                func=self._setup_database_schema
            ),
            Tool(
                name="manage_dependencies",
                description="Manage project dependencies",
                func=self.dependency_manager.manage
            )
        ]
    
    async def _create_api_endpoint(self, spec: Dict[str, Any]) -> str:
        """API ì—”ë“œí¬ì¸íŠ¸ ìƒì„±"""
        template = """
from fastapi import APIRouter, Depends, HTTPException, status
from typing import List, Optional
from sqlalchemy.ext.asyncio import AsyncSession

from src.core.database import get_db
from src.schemas.{schema_name} import {schema_name}Create, {schema_name}Update, {schema_name}Response
from src.services.{service_name} import {service_name}Service

router = APIRouter()

@router.post("/", response_model={schema_name}Response, status_code=status.HTTP_201_CREATED)
async def create_{resource_name}(
    data: {schema_name}Create,
    db: AsyncSession = Depends(get_db)
):
    \"\"\"Create a new {resource_name}\"\"\"
    service = {service_name}Service(db)
    return await service.create(data)

@router.get("/{{{resource_name}_id}}", response_model={schema_name}Response)
async def get_{resource_name}(
    {resource_name}_id: str,
    db: AsyncSession = Depends(get_db)
):
    \"\"\"Get a {resource_name} by ID\"\"\"
    service = {service_name}Service(db)
    result = await service.get({resource_name}_id)
    if not result:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="{resource_name} not found"
        )
    return result
"""
        
        # í…œí”Œë¦¿ ì±„ìš°ê¸°
        code = template.format(
            schema_name=spec["schema_name"],
            service_name=spec["service_name"],
            resource_name=spec["resource_name"]
        )
        
        # íŒŒì¼ ìƒì„±
        file_path = f"src/api/routes/{spec['resource_name']}.py"
        await self._mcp_filesystem_tool("write", file_path, code)
        
        return f"API endpoint created at {file_path}"
    
    async def _create_react_component(self, spec: Dict[str, Any]) -> str:
        """React ì»´í¬ë„ŒíŠ¸ ìƒì„±"""
        template = """
import React, { useState, useEffect } from 'react';
import {{ {imports} }} from '@/components/ui';
import {{ use{hook_name} }} from '@/hooks';

interface {component_name}Props {{
  {props}
}}

export const {component_name}: React.FC<{component_name}Props> = ({{ {prop_names} }}) => {{
  const [state, setState] = useState({initial_state});
  const {{ data, loading, error }} = use{hook_name}();
  
  useEffect(() => {{
    // Component logic
  }}, []);
  
  if (loading) return <div>Loading...</div>;
  if (error) return <div>Error: {{error.message}}</div>;
  
  return (
    <div className="{class_name}">
      {{/* Component JSX */}}
    </div>
  );
}};
"""
        
        code = template.format(
            component_name=spec["component_name"],
            imports=spec.get("imports", "Button, Card"),
            hook_name=spec.get("hook_name", "Query"),
            props=spec.get("props", ""),
            prop_names=spec.get("prop_names", ""),
            initial_state=spec.get("initial_state", "{}"),
            class_name=spec.get("class_name", "container")
        )
        
        file_path = f"frontend/src/components/{spec['component_name']}.tsx"
        await self._mcp_filesystem_tool("write", file_path, code)
        
        return f"React component created at {file_path}"
    
    async def _setup_database_schema(self, spec: Dict[str, Any]) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ì •"""
        # SQLAlchemy ëª¨ë¸ ìƒì„±
        model_code = f"""
from sqlalchemy import Column, String, DateTime, Boolean, ForeignKey
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship
import uuid
from datetime import datetime

from src.storage.database import Base

class {spec['model_name']}(Base):
    __tablename__ = "{spec['table_name']}"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    {self._generate_columns(spec['columns'])}
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
"""
        
        file_path = f"src/storage/models/{spec['model_name'].lower()}.py"
        await self._mcp_filesystem_tool("write", file_path, model_code)
        
        # Alembic ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±
        migration_command = f"alembic revision --autogenerate -m 'Add {spec['model_name']} model'"
        await self._mcp_filesystem_tool("execute", ".", migration_command)
        
        return f"Database schema created for {spec['model_name']}"
    
    def _generate_columns(self, columns: List[Dict[str, Any]]) -> str:
        """ì»¬ëŸ¼ ì •ì˜ ìƒì„±"""
        column_defs = []
        
        for col in columns:
            col_type = col['type']
            col_name = col['name']
            nullable = col.get('nullable', True)
            
            if col_type == 'string':
                col_def = f"{col_name} = Column(String({col.get('length', 255)}), nullable={nullable})"
            elif col_type == 'boolean':
                col_def = f"{col_name} = Column(Boolean, default={col.get('default', False)})"
            elif col_type == 'datetime':
                col_def = f"{col_name} = Column(DateTime, nullable={nullable})"
            elif col_type == 'uuid':
                col_def = f"{col_name} = Column(UUID(as_uuid=True), ForeignKey('{col.get('foreign_key')}'), nullable={nullable})"
            else:
                col_def = f"{col_name} = Column(String(255), nullable={nullable})"
            
            column_defs.append(col_def)
        
        return '\n    '.join(column_defs)
    
    async def _process_result(self, result: Any) -> Dict[str, Any]:
        """ê°œë°œ ê²°ê³¼ ì²˜ë¦¬"""
        return {
            "generated_files": result.get("files", []),
            "dependencies": result.get("dependencies", {}),
            "api_endpoints": result.get("endpoints", []),
            "database_models": result.get("models", []),
            "documentation": result.get("documentation", "")
        }
```

## Phase 4: í†µí•© ë° ìµœì í™”

### 4.1 API ì„œë²„ êµ¬í˜„ (3ì¼)

```python
# src/api/main.py
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import time

from src.api.routes import pipeline, agents, monitoring
from src.core.config import get_settings
from src.api.middleware.auth import AuthMiddleware
from src.api.middleware.logging import LoggingMiddleware
from src.api.middleware.error import error_handler
from src.utils.logger import setup_logging

# ì„¤ì • ë° ë¡œê¹…
settings = get_settings()
setup_logging()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    openapi_url=f"{settings.api_v1_str}/openapi.json"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì»¤ìŠ¤í…€ ë¯¸ë“¤ì›¨ì–´
app.add_middleware(AuthMiddleware)
app.add_middleware(LoggingMiddleware)

# ì—ëŸ¬ í•¸ë“¤ëŸ¬
app.add_exception_handler(Exception, error_handler)

# ë¼ìš°í„° ë“±ë¡
app.include_router(
    pipeline.router,
    prefix=f"{settings.api_v1_str}/pipelines",
    tags=["pipelines"]
)
app.include_router(
    agents.router,
    prefix=f"{settings.api_v1_str}/agents",
    tags=["agents"]
)
app.include_router(
    monitoring.router,
    prefix=f"{settings.api_v1_str}/monitoring",
    tags=["monitoring"]
)

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì´ë²¤íŠ¸"""
    # ì›Œí¬í”Œë¡œìš° ì—”ì§„ ì´ˆê¸°í™”
    from src.orchestration.engine import workflow_engine
    from src.orchestration.graphs.main import create_main_workflow
    
    # ê·¸ë˜í”„ ë“±ë¡
    workflow_engine.register_graph("main", create_main_workflow())
    
    # MCP ì„œë²„ ì‹œì‘
    from src.integrations.mcp import mcp_manager
    await mcp_manager.start_all_servers()

@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì´ë²¤íŠ¸"""
    # MCP ì„œë²„ ì •ë¦¬
    from src.integrations.mcp import mcp_manager
    await mcp_manager.stop_all_servers()

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "name": settings.app_name,
        "version": settings.app_version,
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "timestamp": time.time()
    }
```

### 4.2 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (2ì¼)

```python
# src/utils/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Info
from functools import wraps
import time

# ë©”íŠ¸ë¦­ ì •ì˜
pipeline_counter = Counter(
    'pipeline_total',
    'Total number of pipeline executions',
    ['status', 'type']
)

pipeline_duration = Histogram(
    'pipeline_duration_seconds',
    'Pipeline execution duration',
    ['type']
)

active_pipelines = Gauge(
    'active_pipelines',
    'Number of currently active pipelines'
)

agent_execution_counter = Counter(
    'agent_execution_total',
    'Total number of agent executions',
    ['agent', 'status']
)

token_usage_counter = Counter(
    'token_usage_total',
    'Total token usage',
    ['model', 'type']
)

system_info = Info(
    'system',
    'System information'
)

def track_pipeline_execution(pipeline_type: str):
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            active_pipelines.inc()
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                pipeline_counter.labels(status='success', type=pipeline_type).inc()
                return result
            except Exception as e:
                pipeline_counter.labels(status='failure', type=pipeline_type).inc()
                raise
            finally:
                duration = time.time() - start_time
                pipeline_duration.labels(type=pipeline_type).observe(duration)
                active_pipelines.dec()
        
        return wrapper
    return decorator

def track_agent_execution(agent_name: str):
    """ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            try:
                result = await func(*args, **kwargs)
                agent_execution_counter.labels(agent=agent_name, status='success').inc()
                return result
            except Exception as e:
                agent_execution_counter.labels(agent=agent_name, status='failure').inc()
                raise
        
        return wrapper
    return decorator

# ì‹œìŠ¤í…œ ì •ë³´ ì„¤ì •
system_info.info({
    'version': '1.0.0',
    'environment': 'production'
})
```

## Phase 5: í”„ë¡œë•ì…˜ ì¤€ë¹„

### 5.1 í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ (3ì¼)

```python
# tests/unit/test_workflow_engine.py
import pytest
from unittest.mock import Mock, AsyncMock

from src.orchestration.engine import WorkflowEngine
from src.orchestration.state import StateManager, TaskType

@pytest.fixture
async def workflow_engine():
    """ì›Œí¬í”Œë¡œìš° ì—”ì§„ í”½ìŠ¤ì²˜"""
    engine = WorkflowEngine()
    yield engine
    # ì •ë¦¬
    await engine.cleanup()

@pytest.mark.asyncio
async def test_workflow_execution(workflow_engine):
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
    # í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ ìƒì„±
    from langgraph.graph import StateGraph
    
    graph = StateGraph(dict)
    graph.add_node("test", lambda x: {"result": "success"})
    graph.set_entry_point("test")
    graph.add_edge("test", END)
    
    # ê·¸ë˜í”„ ë“±ë¡
    workflow_engine.register_graph("test", graph)
    
    # ì‹¤í–‰
    result = await workflow_engine.execute(
        "test",
        {"input": "test_data"}
    )
    
    assert result["result"] == "success"

@pytest.mark.asyncio
async def test_state_management():
    """ìƒíƒœ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""
    state = StateManager.create_initial_state(
        "test-pipeline",
        "Test requirements",
        TaskType.FEATURE
    )
    
    assert state["pipeline_id"] == "test-pipeline"
    assert state["task_type"] == TaskType.FEATURE
    assert state["status"] == "initialized"
    
    # ë©”ì‹œì§€ ì¶”ê°€
    StateManager.add_message(state, "TestAgent", "Test message")
    assert len(state["messages"]) == 1
    assert state["messages"][0].agent == "TestAgent"
```

### 5.2 ë°°í¬ ì„¤ì • (2ì¼)

#### Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agentic-pipeline
  labels:
    app: agentic-pipeline
spec:
  replicas: 3
  selector:
    matchLabels:
      app: agentic-pipeline
  template:
    metadata:
      labels:
        app: agentic-pipeline
    spec:
      containers:
      - name: app
        image: agentic-pipeline:latest
        ports:
        - containerPort: 8080
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: agentic-secrets
              key: database-url
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: agentic-secrets
              key: anthropic-api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

### 5.3 ë¬¸ì„œí™” (2ì¼)

```python
# scripts/generate_docs.py
import os
import subprocess
from pathlib import Path

def generate_api_docs():
    """API ë¬¸ì„œ ìƒì„±"""
    # OpenAPI ìŠ¤í™ ì¶”ì¶œ
    subprocess.run([
        "python", "-c",
        "from src.api.main import app; import json; "
        "print(json.dumps(app.openapi()))"
    ], stdout=open("docs/openapi.json", "w"))
    
    # Redoc ìƒì„±
    subprocess.run([
        "npx", "redoc-cli", "bundle",
        "docs/openapi.json",
        "-o", "docs/api.html"
    ])

def generate_code_docs():
    """ì½”ë“œ ë¬¸ì„œ ìƒì„±"""
    subprocess.run([
        "sphinx-build", "-b", "html",
        "docs/source", "docs/build"
    ])

if __name__ == "__main__":
    generate_api_docs()
    generate_code_docs()
    print("Documentation generated successfully!")
```

## ë¦¬ìŠ¤í¬ ê´€ë¦¬

### ê¸°ìˆ ì  ë¦¬ìŠ¤í¬
| ë¦¬ìŠ¤í¬ | ì˜í–¥ë„ | í™•ë¥  | ëŒ€ì‘ ë°©ì•ˆ |
|--------|--------|------|-----------|
| Claude API ì œí•œ | ë†’ìŒ | ì¤‘ê°„ | ìš”ì²­ ì œí•œ ë° ìºì‹± êµ¬í˜„ |
| LangGraph ì„±ëŠ¥ ì´ìŠˆ | ì¤‘ê°„ | ë‚®ìŒ | ë…¸ë“œ ìºì‹± ë° ë³‘ë ¬ ì²˜ë¦¬ |
| MCP ì„œë²„ ì•ˆì •ì„± | ì¤‘ê°„ | ì¤‘ê°„ | í—¬ìŠ¤ì²´í¬ ë° ìë™ ì¬ì‹œì‘ |
| ëŒ€ìš©ëŸ‰ ìƒíƒœ ê´€ë¦¬ | ë†’ìŒ | ë‚®ìŒ | ì™¸ë¶€ ìŠ¤í† ë¦¬ì§€ í™œìš© |

### ìš´ì˜ì  ë¦¬ìŠ¤í¬
| ë¦¬ìŠ¤í¬ | ì˜í–¥ë„ | í™•ë¥  | ëŒ€ì‘ ë°©ì•ˆ |
|--------|--------|------|-----------|
| ë¹„ìš© ì´ˆê³¼ | ë†’ìŒ | ì¤‘ê°„ | ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì˜ˆì‚° ì•Œë¦¼ |
| ë³´ì•ˆ ì·¨ì•½ì  | ë†’ìŒ | ë‚®ìŒ | ì •ê¸°ì  ë³´ì•ˆ ìŠ¤ìº” |
| ë°ì´í„° ì†ì‹¤ | ë†’ìŒ | ë‚®ìŒ | ë°±ì—… ë° ë³µêµ¬ ì „ëµ |

## ì„±ê³µ ì§€í‘œ

### ê¸°ìˆ ì  KPI
- **ì‘ë‹µ ì‹œê°„**: 95%ile < 5ì´ˆ
- **ê°€ìš©ì„±**: 99.9% ì´ìƒ
- **ì—ëŸ¬ìœ¨**: < 0.1%
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 85% ì´ìƒ

### ë¹„ì¦ˆë‹ˆìŠ¤ KPI
- **ê°œë°œ ì‹œê°„ ë‹¨ì¶•**: 70% ì´ìƒ
- **ì½”ë“œ í’ˆì§ˆ**: SonarQube Aë“±ê¸‰
- **ì‚¬ìš©ì ë§Œì¡±ë„**: NPS 50 ì´ìƒ
- **ë¹„ìš© ì ˆê°**: 30% ì´ìƒ

### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì 
- ì•Œë¦¼ ì„¤ì • (Slack, Email)
- ì›”ê°„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±

## êµ¬í˜„ ì¼ì • ìš”ì•½

| Phase | ê¸°ê°„ | ì£¼ìš” ê²°ê³¼ë¬¼ |
|-------|------|-------------|
| Phase 1: ê¸°ë°˜ ì¸í”„ë¼ | 2ì£¼ | í”„ë¡œì íŠ¸ êµ¬ì¡°, ê°œë°œ í™˜ê²½, DB ìŠ¤í‚¤ë§ˆ |
| Phase 2: í•µì‹¬ ì—”ì§„ | 3ì£¼ | LangGraph ì—”ì§„, ìƒíƒœ ê´€ë¦¬, ê¸°ë³¸ ë…¸ë“œ |
| Phase 3: ì—ì´ì „íŠ¸ | 3ì£¼ | 5ê°œ ì „ë¬¸ ì—ì´ì „íŠ¸ êµ¬í˜„ |
| Phase 4: í†µí•© | 2ì£¼ | API ì„œë²„, ëª¨ë‹ˆí„°ë§, ìµœì í™” |
| Phase 5: í”„ë¡œë•ì…˜ | 2ì£¼ | í…ŒìŠ¤íŠ¸, ë°°í¬, ë¬¸ì„œí™” |

**ì´ ì†Œìš” ê¸°ê°„: 12ì£¼**

## ë‹¤ìŒ ë‹¨ê³„

1. **PoC êµ¬í˜„** (2ì£¼)
   - í•µì‹¬ ê¸°ëŠ¥ë§Œ êµ¬í˜„
   - ê¸°ìˆ  ê²€ì¦ ìˆ˜í–‰

2. **MVP ê°œë°œ** (6ì£¼)
   - ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° êµ¬í˜„
   - 2ê°œ ì—ì´ì „íŠ¸ êµ¬í˜„

3. **ì ì§„ì  í™•ì¥** (4ì£¼)
   - ë‚˜ë¨¸ì§€ ì—ì´ì „íŠ¸ ì¶”ê°€
   - ì„±ëŠ¥ ìµœì í™”

4. **í”„ë¡œë•ì…˜ ë°°í¬**
   - ë‹¨ê³„ì  ë¡¤ì•„ì›ƒ
   - ëª¨ë‹ˆí„°ë§ ê°•í™”